{"index":{"title":"üìù Daniel's Notes.","links":[],"tags":[],"content":"Welcome to my public notes. Not quite a blog, but not a timeline either. This kind of personal wiki of linked notes is my attempt to gradually move away from social media platforms but still keep some kind of connection with the public internet.\nThis ‚Äúnet of notes‚Äù is very close to the idea of a digital garden, in that it takes inspiration from the early web; from hypertext and the Memex, and from keeping some kind of ownership of the files one shares. In keeping with these ideas, the notes in this garden are often drafts that are meant to grow (or wither) organically. Some notes might make it to my main site as they grow into projects or publications, many will remain here while they grow.\nYou can read A Brief History &amp; Ethos of the Digital Garden, and this piece on The Garden and the Stream to find out more about digital gardens.\n"},"home/daniel/OneDrive/Academic/Research/Publications/first_cow/Cinema-and-Machine-Vision/projects/DV/data/shots/stills/godfather-Scene-1365-03.jpg":{"title":"godfather-Scene-1365-03.jpg","links":[],"tags":[],"content":""},"public_notes/5-principles-of-life":{"title":"5 principles of life","links":["public_notes/Cultural-Metabolism-and-LLMs-on-wheels"],"tags":["complex-systems","cellular-biology"],"content":"\nSome very high-level background about cell-biology for my talk on  Cultural Metabolism and LLMs on wheels at Prague earlier this year.\nAccording to Paul Nurse:\n\nLiving things are bounded physical entities\nThe bounded entity is the chemical and informational machine\nThis chemical and informational machine has a hereditary system\n\nThe system prefigures how the entity works\nBut has in-built variability to respond to its environment (epigenetics)\n\n\nThe entity can therefore evolve by natural selection\nThe living thing can have purpose of being better adapted to the life state it finds itself\n\nFrom his book What is Life?"},"public_notes/Cinema-and-Machine-Vision":{"title":"Cinema and Machine Vision","links":["public_notes/Tools/Movie-Barcodes"],"tags":[],"content":"\nI am very happy that my book is coming coming out in 2024 with Edinburgh University Press\nHere a teaser from the back cover:\n\nCinema and Machine Vision unfolds the aesthetic, epistemic, and ideological dimensions of machine-seeing films and television using computers. With its critical-technical approach, this book presents to the reader key new problems that arise as AI becomes integral to visual culture. It theorises machine through a selection of aesthetics, film theory, and applied machine learning research, dispelling widely held assumptions about computer systems designed to watch and make images on our behalf.¬†\n\nAnd here‚Äôs something for those who prefer to judge a book by its cover."},"public_notes/Creanalytics":{"title":"Creanalytics","links":["public_notes/Tools/Videogrep"],"tags":["concepts","publications"],"content":"\nCreanalytics: a portmanteau of creative and analytic.\nThis idea came up when writing about the computational video essay, largely in response to creative and analytic research methods coexisting under a unified computational framework in the age of generative AI. This could be understood as generation through prediction, similar to the analytic-synthetic distinction which I found in this piece about analytic augmentation in prompt engineering.\nUpdate: this is now an article! Forthcoming in a special Issue on critical technical practice in the journal Convergence: The International Journal of Research into New Media Technologies\n\nAlso related to a project in collaboration with my wonderful colleagues Jonathan Gray and Mercedes Bunz on using video editing techniques as data analysis. We commissioned Sam Lavigne to work on  his Videogrep tool! More on this soon!"},"public_notes/Creative-AI-Theory-and-Practice":{"title":"Creative AI Theory and Practice","links":[],"tags":["cail","event","ai"],"content":"\nThe recording of our symposium on Creative AI is now available. Presentations are indexed at different timestamps.\n\nOn Friday 27 January 2023, Creative AI Lab at King‚Äôs College London/Serpentine (Professor Mercedes Bunz and curator Eva J√§ger as Lab‚Äôs co-founders, Dr Daniel Ch√°vez Heras, PhD student Alasdair Milne, Professor Joanna Zylinska) hosted a one-day symposium supported by the King‚Äôs Institute for Artificial Intelligence.\n\n"},"public_notes/Cultural-Metabolism-and-LLMs-on-wheels":{"title":"NYU - Prague","links":["tags/llms"],"tags":["cail","llms","ai","paper","presentation"],"content":"\nI was invited by the great people of the Digital Theory Lab at NYU to present this in Prague this summer. Very much looking forward to it!\nCultural Metabolism and LLMs on wheels ¬ß\nAbstract ¬ß\nLarge Language Models (#llms ) trained on vast amounts of internet text are able to simulate natural language structures to a degree that enables novel interfaces for human-computer interaction. One key function in these systems can be observed at the micro level, when the next token in an unfolding series is predicted and concatenated recursively, in other words, at the logical step when calculation becomes synthesis.\nIn this presentation I focus on this very localised but crucial moment of exchange, drawing a parallel with metabolic processes in cellular biology, to explore the notion of self-contained and embedded autonomous systems: LLMs+ or LLMs on wheels.\nThis is related to my interest in agent-based modelling and complex systems in cultural domains. Inspired among other influences by Thilo Gros and his work at Biond Lab\nI love his complexity papers videos!\n"},"public_notes/EINA/index":{"title":"EINA conference","links":[],"tags":["presentation"],"content":"\n ¬ß\nBienes p√∫blicos, ganancias privadas ¬ß\ncreatividad y producci√≥n digital en la era de los medios sint√©ticos ¬ß\nDr Daniel Ch√°vez Heras\nKing‚Äôs College London\n@chavezheras@sigmoid.social\nmovingpixel.net\nnote: gracias por la invitaci√≥n, enctantado de estar aqu√≠.\nPrograma del d√≠a ‚Äî&gt; un poco sobre mi contexto y mi trabajo como parte del creative AI Lab, y profundizar en un proyecto y aspecto espec√≠fico relacionado con medios sint√©ticos y producci√≥n creativa\n\n\nnote: Mi rol es Catedr√°tico e investigador en el Departamento de Humanidades Digitales m√°s grade en su tipo, colegas en todas las √°reas cl√°sicas de las artes y las humanidades: historia, geograf√≠a, pol√≠tica; arte, literatura, y en mi caso cine.\n\nPreguntas existentes con m√©todos nuevos ¬ß\nPreguntas nuevas con marcos te√≥ricos existentes ¬ß\n\nPreservaci√≥n y curadur√≠a digital\nEst√°ndares abiertos y reg√≠menes de datos\nImpacto social y cultural de nuevas tecnolog√≠as\n\n\nCultura Digital + Creatividad Computacional ¬ß\n\nComputational Humanities Research Group\nCreative AI lab\n\nnote: Dos grupos de investigaci√≥n ‚Äî&gt; especialidad en cultura digital (an√°lisis de fen√≥menos culturales mediante m√©todos computacionales) y creatividad computacional (definiri aqu√≠).\nMi l√≠nea de investigaci√≥n vincula estas dos √°reas, y se enfoca en la familia de tecnolog√≠as com√∫nmente conocidas como Inteligencia Artificial: su historia y naturaleza como t√©cnica, sus aplicaciones como herramienta anal√≠tica y creativa, y sus implicaciones sociales y culturales.\n\nDepartamento de Humanidades Digitales + Serpentine Galleries ¬ß\n\nnote: proyecto de colaboraci√≥n entre DDH y Serpentine Galleries en Londres.\nIniciado en 2019 (?) por por mis colegas: Mercedes Bunz, en King‚Äôs, y Eva Jager, curadora de programas digitales en Serpentine.\n\nCreative AI Lab ¬ß\nEspacio de colaboraci√≥n y red de intercambio entre artistas, curadores y acad√©micos ¬ß\n\nSeminarios de investigaci√≥n\nSimposio sobre IA y creatividad\nPublicaciones (creative-ai.org), podcast\nProyectos\n\nnote: un esfuerzo por agregar y mapear algunas de las herramientas, tecnol√≥gicas y conceptuales relacionadas con los usos creativos y art√≠sticos de tecnolog√≠as de IA.\nRepositorio de investigaci√≥n. Trabajo curatorial: Eva y Alasdair\n\n\n\n\n\n\n\n\n\nPublic AI ¬ß\n\nnote: Serpentine y KCL son organizaciones con financiamiento p√∫blico\nVisi√≥n multidimensional de IA para sectores creativos.\nM√°s all√° de los derechos de autor sobre inputs (datasets) y outputs (texto o im√°genes generadas), desglosamos las distintas capas de la infraestructura tecnol√≥gica: desde la capa de aplicaciones, y herramientas que ven los usuarios finales, los modelos computacionales, la infraestructura de almacenamiento y c√≥mputo de alto rendimiento, y por supuesto la energ√≠a necesaria para mantener estos sistemas.\n\nTransformaci√≥n de los medios de producci√≥n ¬ß\nPerspectivas desde las industrias creativas ¬ß\n]\nnote: imagen para el dise√±o de portada de este libro, hecho en 2021, publicado en 2022.\nExperimentando con VQGAN, Disco Diffusion, y otros modelos para generaci√≥n de im√°genes guiados por CLIP (a su vez otro modelo desarrollado por Open AI).\n\n\nThe final image called for human estimation, approximation, and simple guesswork. It was more work than creativity. Producing the image additionally required infrastructure credits from Google. The book cover therefore is an expression of one of the key ideas in this book: that we need to understand algorithmic operations and AI from the perspective of work.\n\n- Tobias Blanke\nnote: en los agradecimientos del libro, Tobias detalla el proceso para la generaci√≥n de la im√°gen.\n\n\nnote: Para mi propio libro, Cinema and Machine Vision, pr√≥ximo a publicarse con Edinburgh University Press, apenas dos a√±os despu√©s, suger√≠ una portada generada con una de estas t√©cnicas, y EUP amablemente explicaron que no era posible por temas de derechos de autor. Se sorprendieron cuando les dije que OUP ya lo hab√≠a hecho!\n\n¬øQu√© sucedi√≥? ¬ß\n¬øC√≥mo pasamos de experimentos de garage a medios masivos a escala industrial? ¬ß\nnote: de un grupo de entusiastas experimentando con nuevas tecnolog√≠as que en apariencia no ten√≠an una aplicaci√≥n obvia, a empresas multimillonarias que ahora absorben otras industrias completas, como el caso de las im√°genes de stock, o son objeto de  demandas, como el caso de Getty Images, o la acci√≥n colectiva de demanda por un grupo de artistas en contra de Stability AI.\n\nUn poco de historia ¬ß\n\nnote: En realidad esto no sucedi√≥ de la noche a la ma√±ana, la transformaci√≥n en los medios de producci√≥n tiene alg√∫n tiempo gest√°ndose, y se manifiesta ahora de manera m√°s visible en las industrias creativas-cognitivas que hab√≠an hasta ahora permanecido m√°s al margen de la automatizaci√≥n industrial.\nEl caso paradigmatico de ImageNet.\nEl ImageNet challenge en 2012 abri√≥ la puerta para un nuevo paradigma conocido como ‚Äúdeep learning‚Äù basado en redes neuronales artificiales, paradigma que hoy domina lo que llamamos com√∫nmente IA.\nPero hay otras IAs!\n\nFlickr ¬ß\n\nnote:\nPhilip Howard, who uploaded it to the platform in 2013 as part of a series of black and white photographs depicting the organised washing of circus elephants in the seaside town of Weymouth, in Britain, around 1987\nMillones de im√°genes como esta fueron recolectadas por sus descripciones ‚Äútags‚Äù en un ejercicio automatizado de captura. Clave para ello es que las im√°genes est√°n disponibles en l√≠nea de manera p√∫blica, pueden encontrarse y clasificarse de manera autom√°tica utilizando sus descripciones y tags, y tienen licencias de Creative Commons.\n\nMS Coco ¬ß\n\nnote: de ImageNet, muchas acabaron en otros datasets, como MS COCO (Common Objects in Context)\n\nDenseCap ¬ß\n\nnote: o DenseCap, desarrollado por Fei Fei Li para automatizar la descripci√≥n detallada de im√°genes\n\nCompartir y trabajar ¬ß\n\nnote: cuando Philip Howard anot√≥ su fotograf√≠a, nunca consider√≥ que esas breves anotaciones podr√≠an ser utilizadas para entrenar Inteligencia Artificial.\nParece trivial, porque podemos describir lo que vemos de manera instant√°nea, pero en realidad esta habilidad descansa en millones de a√±os de evoluci√≥n, depende por ejemplo, del desarrollo del lenguaje, y de otras tecnolog√≠as de representaci√≥n, como le escritura y la fotograf√≠a\n\nCinco etapas en la historia reciente de tecnolog√≠as de IA ¬ß\n\nModificaci√≥n de los patrones de trabajo intelectual y cognitivo.\nDigitalizaci√≥n de productos culturales y cognitivos.\nCaptura a gran escala de bienes comunes (CC).\nPrivatizaci√≥n de datos como recursos para el desarrollo de IA.\nComercializaci√≥n de IA como servicio de renta.\n\nnote: podr√≠amos ir m√°s lejos, por ejemplo, para entender como es que estas tecnolog√≠as de vision computacional est√°n construidas sobre reg√≠menes t√©cnicos anteriores, como la fotograf√≠a o la imprenta. Pero por ahora, ah√≠ lo dejamos.\n\n¬øQu√© sigue? ¬ß\n¬øUtop√≠a o apocalipsis? ¬ß\n¬øEmancipaci√≥n creativa o rentismo extractivista? ¬ß\n¬øSimbiosis o parasitismo? ¬ß\nnote: Inundar internet de medios sint√©ticos, o proscribirlos?\nControversias varias sobre uso responsable, √©tica en IA, alineamiento, riesgo existencial, demandas por violaci√≥n a los derechos de autor, moratorium en desarrollo etc.\nLa realidad es m√°s humana y m√°s familiar de lo que estos escenarios sugieren.\n\n\nLo que hoy llamamos inteligencia artificial, opera a trav√©s del tejido social, la econom√≠a, y la cultura. Es lo que podr√≠amos llamar una t√©cnica cultural, en tanto reconfigura el acceso a los medios de producci√≥n, reorganiza el trabajo, y modifica las nociones intersubjetivas de valor.\n\nnote: en este sentido, IA no es tan distinta a otras tecnolog√≠as de uso general, con aplicaciones e implicaciones de largo alcance que dan forma al ecosistema material humano, como la invenci√≥n de la electricidad, el concreto reforzado, el pl√°stico, y la escritura o la fotograf√≠a.\nHay mucho que aprender sobre c√≥mo nacieron, crecieron y se regularon estos avances cient√≠ficos y tecnol√≥gicos, sobre sus efectos sociales y ecol√≥gicos.\n\nUna propuesta en tres partes ¬ß\n\nIdentificar de manera precisa los distintos modelos de producci√≥n en tecnolog√≠as de IA.\nEvaluar cu√°les son los modelos que enriquecen o empobrecen el dominio p√∫blico a largo plazo.\nNegociar en bloque el acceso y gobierno de los nuevos medios de producci√≥n.\n\nnote: inevitablemente habr√° que pasar y en su caso reformar las instituciones del estado y la sociedad civil para dar forma a esta negociaci√≥n, con distintos actores, como las grandes corporaciones (FANGS), pero tambi√©n con otros estados y en un contexto geo‚Äôpol√≠tico de poca confianza y alto riesgo, donde adem√°s los procesos democr√°tizantes posteriores a la guerra fr√≠a est√°n en crisis.\nNo cometer los errores del pasado: regalar algo muy valioso como nuestros datos personales en conjunto con nuestras preferencias y habilidades cognitivas, emocionales, y est√©ticas,  a cambio de conveniencia m√≠nima y gratificaci√≥n inmediata.\nEvitar la privatizaci√≥n de las ganancias y la socializaci√≥n de los riesgos ‚Äî&gt; Mazucatto\nDoctrinas filos√≥ficas para prevenir y mitigar el da√±o individual (John Stuart Mill) ‚Äî&gt; prevenir y mitigar la erosi√≥n de la esfera p√∫blica (Habermass) pero que es adem√°s el concepto clave detr√°s de los derechos de autor y la noci√≥n de enriquecimiento com√∫n.\n\n"},"public_notes/EPFL-symposium/Creanalytics":{"title":"Creanalytics","links":[],"tags":["presentation"],"content":"Creanalytics ¬ß\nBetween Data Analysis and Media Synthesis ¬ß\nDr Daniel Ch√°vez Heras\n\n@chavezheras@sigmoid.social\nmovingpixel.net\nnote: Thank you for the invitation, delighted to be here.\n\n\nnote:\nA project in 2018 to ‚Äúmachine-see‚Äù the BBC television archive, and ‚Äúmachine-edit‚Äù new sequences ‚Äútelevision by the meter‚Äù ‚Äïwe jokingly referred to it.\nAired on BBC 4 on September 2018, seen by half a million people in the UK.\nIt included four sections that corresponded to different computational techniques to traverse the archive: including text analysis over subtitled material, object detection, motion estimation (visual energy), and a mixed between the three (what we would call today a multimodal method).\n\n\n2. MbM: Object Detection from Daniel Ch√°vez Heras on Vimeo.\nnote:\nThis was a project in experimental television, in hindsight it was very much of its time, and frankly, as time goes by I am more and more surprised that no one stopped us from doing it. \nIn light of everything that happened after with AI, this project stands a somewhat of a quaint expression of many of the issues discussed yesterday, from access and copyright, to the role of generative technologies and their impact on screen culture today.\n\nAn epistemic gap between archives and datasets ¬ß\nA functional proximity between data analysis and media synthesis ¬ß\nnote:\nIn this talk I want to highlight two key insights I gained from this project:\nBy this I mean the gap between how it is that AI systems contribute to the production of knowledge, what kinds of knowledge, and how this knowledge might be valuable for the understanding of moving images. And how this is radically different from how moving image archives contribute to knowledge and produce value to the societies that keep them.\nBy this I mean the rather unexpected way in which scientific and creative computing are entangled in generative AI systems.\nI will spend a couple of slides on the first point, because I think most of this ground has been covered in one way or another yesterday, and spend some more time on the second premise, including examples.\n\nArchives and Datasets ¬ß\n\nnote:\nArchives are made for humans, datasets are by humans for machines.\nOversimplification: we know machines are made by and for humans too, but still, the purpose of their\nArchives are created under a historical impulse; they are organised according to the record-keeping needs of the cultures that build them. This historical impulse requires system that facilitates cataloguing and retrieval, and that aspires to a certain degree of historical accuracy, integrity, and permanence.\nDatasets also respond to the sense-making needs of the cultures that build them, but they come together to lay claim on the future more than the past, usually in response to specific problems and questions that need solving, which is to say they are much more instrumental.\nIn data science and machine learning engineering, datasets tend to be granular, flattened to matrix-like structures whose individual items are not meant to be publicly accessible or even individually meaningful to human observers.\nArguably, contemporary AI has succeeded precisely for not caring at all about whether specific media artefacts are deemed significant enough to go ‚Äòon the record‚Äô, and be keept for posterity, with all the cost implications that this kind of collective memory keeping entails, but because of the opposite approach, by voraciously ingesting heaps of data that in themselves were not canonical or significant.\nI would got a step further still, and say that archives and datasets produce value in almost opposite ways: while archives endow their constituent artefacts and records with additional symbolic layers, making them stable and tractable, datasets that feed contemporary AI systems atomise these artefacts, stripping them from context in order to make patterns visible through computational processing. In the first case value is produced by stability and addressability, in the second by aggregation and mutability.\n‚Äî\nData palimpsests ¬ß\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtefactProcessing levelExampleCinemaSocial ‚Äì aggregatePopular Hollywood cinemaFilmHumanJurassic Park (1993)ClipHuman/computerRaptors in the Kitchen Scene (https://youtu.be/dnRxQ3dcaQk)ShotHuman-computer130 frames (5.421 s)FrameComputer/humanIndividual frame (512 √ó 340 pixels)PixelsNumeric ‚Äì disaggregateVector ([176800x1]); Tensor ([16, 3, 340, 512])\nnote:\nYet, there is a relation between archives and datasets.\nEach of the films contained in a film archive can be thought of a dataset of frames, and every frame as dataset of pixels.  Through computing, individual frames and their pixels can relate much more freely, not only to other frames in the same film, but to a multitude of other frames in a multitude of other films, in high-dimensional spaces where every pixel might be put in contact with any other.\nThis table exemplifies the palimpsest of artefacts and levels of analysis at play between audiovisual archives, comprised of artefacts, and datasets atomised for machine learning operations. The epistemic gap between the two ends of the table are yet to be fully understood in the configuration of a computational archive.\n\nData Analysis and Media Synthesis ¬ß\n\nnote:\nAnalytic and generative approaches in computing tend to be split between scientific and creative domains, with their respective tools and communities of practice.\nDeployed as analytical engines, computers can be used to find patterns across vast collections of imagery, and these patterns are often expressed as relations of proximity in space. We have seen examples of this yesterday in multiple way of projecting archives onto 2d or 3d spaces.\nThe image in this slide shows a t-SNE mapping of a collection of soviet news reels. The dataset and the tool were developed by colleagues in CUDAN from the cultural data analytics lab in Tallinn, Estonia.\nBut to amount to knowledge, these spatial correlations require interpretation and explanation, which require relations of necessity, not just proximity, and tend to unfold sequentially, as researchers, critics and users seek to organise these patterns to infer causal relations and plausible reasons for data objects and events to be organised in space the way they are.\nBy coupling an analytical engine with a generative one, computing can be used to configure narratives about these proximity patterns and enable explanatory propositions through compositional techniques familiar to media scholarship.\n‚Äî\nSupercuts ¬ß\n\nJust as capitalism treated workers as machines as a prelude to workers being replaced by machines, so also supercutters simulate database thinking in apparent anticipation of a moment, perhaps in the near future, when neural networks will be able to search the entirety of digitized film history and create supercuts themselves, automatically.\n\n- Max Tohline, 2021\n\nIn the near future there will be a simple software or app, feeding its algorithm with keywords and other elements of interest, which will automatically generate a perfect supercut of media content of any kind within a blink of an eye.\n\n- Mikl√≥s Kiss, 2013\nnote:\na supercut is an editing technique in which short video clips with common motifs or salient stylistic characteristics are extracted from their original context and are sequenced together in a montage. The commonalities are highlighted through repetition and interpreted by viewers as a form of aboutness, which then becomes the thematic content of the supercut.\nThe supercut entails not simply a mode of editing, but a mode of thinking expressed by a mode of editing.\n‚Äî\nComputational supercuts ¬ß\n\nnote:\nThis was made using a tool called VGREP, developed by artist and creative coder Sam Lavigne as part of a small project funded through a small grant I got last year to explore computational media formats.\nThe tool takes a video file as an input, transcribes its dialogue, and then analyses the text to find common ngrams. These can then selected and edited as a supercut.\nThe lecture was called ‚ÄúModeling Doubt, Coding Humility‚Äù and through this technique I found there were many more mentions of doubt than of coding or humility (the lecture however was ver good).\nI wanted to take this idea a step forward and see if a similar effect could be achieved by operating directly on the images, automating a supercut of visual characteristics.\n‚Äî\nMovie Clips YouTube channel ¬ß\n¬†\nnote:\nAn archive-like collection that is publicly available online: the Movieclips YouTube channel (2006), which serves as a corpus that is both large and consistent enough to be analysed and intervened using computational methods.\nThis channel is operated by the American media company Fandango, which owns the popular review aggregator website Rotten Tomatoes, and the recommender platform Flixter, and which is itself jointly owned by the Warner and Universal media conglomerates.\nIn their YouTube channel Movieclips is described as ‚Äòthe largest collection of licensed movie clips on the web‚Äô. At the time of writing, it had over 58 million subscribers and almost 60 billion aggregated views.\nIn terms of access, these numbers dwarf most film archives, but it is important to note that in terms of breadth and diversity, these movie clips are only one deep but thin slice of global film production, namely, recent Hollywood popular cinema licenced by these large media companies.\n‚Äî\nMovie Clips Corpus ¬ß\n\n2691 clips\n350 films\nFrom 1931 to 2019\n287 unique directors\n\n‚Äî\nPre-trained FER detection ¬ß\n\n‚Äî\nShot scale detector ¬ß\n\n‚Äî\nFeature Engineering ¬ß\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureValuePeople1Scale23.2Inferred motion0.24Scale category‚ÄòCU‚ÄôInferred motion category‚ÄòStable‚ÄôTop emotion‚ÄòAngry‚ÄôTop emotion confidence0.79\n‚Äî\nShot duration distribution ¬ß\n\nnote:\nconventional statistics give overview of this dataset of shots\n‚Äî\nShot scale breakdown ¬ß\n\nnote:\nmore sophisticated tools to do this now, like Cineshot deep learning method to find shot scale in films\n‚Äî\nShot emotion breakdown ¬ß\n\nnote:\nthe obvious caveats of this is the simplification of emotions to few categories, an approach that has been criticised and widely updated in psychology.\n‚Äî\nshot scale supercut ¬ß\n\nBig Angry Faces v.01 from Daniel Ch√°vez Heras on Vimeo.\nnote:\nOn the one  hand the computational supercut involves applying computer vision methods to annotate large collections of moving images to find patterns, much in the guise of cultural analytics.\nBut on the other, as well as plotting these images in space to view them at a distance, to make sense of them we need to reinscribe these patterns in time, giving data objects a duration again, and enabling interpretation through the familiar operations of montage and the syntactic and synoptic apparatus of (dis)continuity as a modality for meaning making and explanation.\nThe kind of reverse editing proposed in the computational supercut links in this way analytical and creative epistemic strategies: knowledge and value created through the making and unmaking of moving imagery; visual culture that feeds AI that feeds back into visual culture.&gt;)\n‚Äî\n\nnote:\nI have experimented further with this type of parametric sampling/editing, and some of these experiments can be found online. If there is enough interest I might develop the technique further and might distribute it as a tool that can be used in other collections.\n‚Äî\nContinuity Matrix (No√´l Burch) ¬ß\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime/SpaceContinuousEllipsis (definite)Ellipsis (indefinite)Time reversal (definite)Time reversal (indefinite)ContinuousCCCECEiCRCRiContiguousCtCCtECtEiCtRCtRiDiscontinuousDCDEDEiDRDRi\nnote:\nBut we know we make sense of images not only by what is shown on screen, but by what is omitted. Arguably, films convey as much meaning through time elision than they do through time recording.\nTherefore, in order to model a ‚Äúcomputational Burch‚Äù we need both the edit points that separate one shot from the next and an annotation of what viewers infer to be in-between the shots. This second signal is much more elusive because it needs to be encoded form a subjective unconscious process that appears to leave no traces.\n‚Äî\n\n‚Äî\n\n‚Äî\n\nnote:\nNote that while the patterns above reflect the parallel editing technique in this sequence, they are not representations of the shots themselves, but of the relations between them as inferred by a viewer; the subjective invisible that makes the objective visible meaningful.\n\nMoving image archive of the future ¬ß\n\nCreate and recreate the archive\nPlay the archive\n\nnote:\n\nStructurally similar to a system of systems\nAccessed through narrative forms of interaction\nShaped and reshaped on demand\n\naccess different versions of the archive, and simulate what might have been under different circumstances\nplay (reproduce) the archive, as if it were a larger form of media\n\n\n\n\n\nnote:"},"public_notes/Human-centered-machine-vision-q":{"title":"Human-centered machine vision?","links":["public_notes/Creanalytics"],"tags":["events","machine-vision","design","critical-technical-practice"],"content":"\nFrom the Click and Collect: Show Me Your Dataset event at Somerset House. I was invited as part of a panel with Charlotte Webb and Kristina Pulejkova to discuss human-centred design in relation to AI.\nI presented ongoing research on Creanalytics forthcoming as an article in a special issue on Critical Technical Practice in the journal Convergence:\n\n\nThe recording of the panel is also now available here starting at about 6:14\n\n\nSee the  twitter thread for more on the larger event.\n"},"public_notes/Sculpting-Time-with-Computers/Andrea-Farina":{"title":"Andrea Farina","links":["tags/computationallinguistics"],"tags":["person","computationallinguistics"],"content":"About ¬ß\nAndrea Farina is a PhD Researcher in Digital Humanities. So far, he has been involved in the syntactic or semantic annotation for the following on-going projects: the Greek WordNet, the Latin WordNet, the Sanskrit WordNet, and the Digital Corpus of Sanskrit. He currently holds a UKRI scholarship from the London Arts &amp; Humanities Partnership (LAHP).\nBackground ¬ß\nLinguistics, Classics.\nField ¬ß\n#computationallinguistics\nInterests/projects/skills ¬ß\n\nComputational tools and methods for Historical Linguistics.\nSyntactic and semantic annotation and quantitative analysis in ancient languages (Latin, Ancient Greek).\nAnalysis of the semantics of pre-verbs via computational methods in ancient languages (Latin, Ancient Greek).\nAnalysis of literal and metaphorical motion events.\n\nLinks ¬ß\nhttps://www.kcl.ac.uk/people/andrea-farina\nhttps://twitter.com/ImAndreaFarina"},"public_notes/Sculpting-Time-with-Computers/Annotation-guidelines":{"title":"Annotation guidelines","links":["public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/Andrea-Farina","public_notes/Sculpting-Time-with-Computers/day-two"],"tags":["stc-workshop"],"content":"\nDuring day two, Mila and Andrea worked on a series of guidelines for human annotation of the the soviet news reels, focusing on scenes or ‚Äúblocs‚Äù, rather than stories.\nTags include:\n\nTime stamps of the blocs\nTopics of the blocs\nSettings or locations\nSound change over blocs\n\nThese were thought as a possible new sources of data from human annotators, but also as sampling guidelines to inform the list of CLIP queries."},"public_notes/Sculpting-Time-with-Computers/Bel√©n-Vidal":{"title":"Bel√©n Vidal","links":["tags/filmstudies"],"tags":["person","filmstudies"],"content":"About ¬ß\nBel√©n Vidal is Reader in Film Studies. She is the author of Figuring the Past: Period Film and the Mannerist Aesthetic (Amsterdam University Press, 2012) and Heritage Film. Nation, Genre and Representation (Columbia University Press/Wallflower Press, 2012). She is the co-editor of The Biopic in Contemporary Film Culture (AFI Film Readers, Routledge 2014) and of Cinema at the Periphery (Wayne State University Press, 2010).\nBackground ¬ß\nShe holds a PhD in Film Studies from Glasgow University, an MA from Georgetown University and a BA (Honors) from the University of Valencia. Before joining King‚Äôs in 2008, she was Lecturer in Film Studies at the University of St Andrews.\nField of study ¬ß\n#filmstudies\nInterests/projects/skills ¬ß\nBel√©n co-leads the international project AGE-C Ageing and Gender in European cinema\n\nAGE-C aims to establish cultural gerontology as a key approach in film studies by training a cohort of postdocs to study how cinematic representations of gender shape notions of old age and well-being across Europe.\n\nLinks ¬ß\nhttps://www.kcl.ac.uk/people/dr-belen-vidal\nResearch Interests¬† ¬ß\nFilm analysis and film theory, focusing on:\n\nHistory and memory in contemporary cinemas\nThe historical film genres, especially the biopic and the heritage film\nSpanish film history, and contemporary cinema &amp; media in Spain\nCinephilia and film cultures\n"},"public_notes/Sculpting-Time-with-Computers/Carlo-Bretti":{"title":"Carlo Bretti","links":["tags/computervision"],"tags":["person","computervision"],"content":"About ¬ß\nI‚Äôm a first-year PhD candidate at the Multimedia Analytics Lab of the University of Amsterdam under the supervision of Nanne van Noord and Pascal Mettes.\nI‚Äôm currently conducting research on computer vision and deep learning for film production and analysis. Before starting at MultiX, I worked on automatic trailer generation at the Video &amp; Image Sense Lab. Prior to that, I wrote my thesis on zero-shot action recognition in videos using diverse sets of object-scene compositions, advised by Pascal Mettes.\nBackground ¬ß\n\n\nMSc Data Science, 2021\nUniversity of Amsterdam\n\n\nBSc Communication Science, 2020\nUniversity of Amsterdam\n\n\nField ¬ß\n#computervision\nInterests/projects/skills ¬ß\n\nComputer Vision\nCreative AI\nVideo Understanding\n\nLinks ¬ß\nhttps://carlobretti.github.io/"},"public_notes/Sculpting-Time-with-Computers/High-dimensional-Cinema":{"title":"High-dimensional Cinema","links":["public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/Sculpting-Time-With-Computers","public_notes/Sculpting-Time-with-Computers/Bel√©n-Vidal"],"tags":["stc-workshop"],"content":"\nNanne van Noord, Mila Oiva and myself got together to to present recent research and engage in conversation about recent advances at the intersection between cultural analytics, computational aesthetics, and machine learning. This was a public-facing event part of the the Sculpting Time With Computers workshop, supported by the Department of Digital Humanities and the Digital Futures Institute at King‚Äôs College London.\n\nThe panel was chaired by Bel√©n Vidal. See the full recording below and here are the timestamps for each section:\n\n00:00:00: Introduction DFI (Marion Thain)\n00:04:25: Introduction DFI research (Steven Connor)\n00:08:25 : Nanne van Noord ‚Äï An Analytics of Film Culture\n00:21:12: Mila Oiva ‚Äï Exploring Soviet Newsreels with Computer Vision\n00:34:00: Daniel Ch√°vez Heras ‚Äï Creanalytics: between analysis and generation\n00:49:34: Q&amp;A - New answers to old questions, or new questions through new methods?\n01:01:50: Q&amp;A - Perestroika problem or feature?\n01:07:00: Q&amp;A - Applications beyond academia?\n01:13:15: Q&amp;A - Challenges and opportunities of multi-modal computational methods?\n01:18:34: Q&amp;A - Preserving or contesting the cannon through computers?\n01:27:56: Q&amp;A - Revising theory through critical technical practice\n\n"},"public_notes/Sculpting-Time-with-Computers/Isadora-Campregher":{"title":"Isadora Campregher","links":["tags/filmstudies","tags/filmindustries"],"tags":["person","filmstudies","filmindustries"],"content":"About ¬ß\nResearch Associate and Lecturer at Goethe University. Research associate @DiciHub. Producer @Authoredby_AI. Amateur data scientist.\nBackground ¬ß\nInternational Relations and MA in Sociology with a focus on Gender Studies from Universidade Federal do Rio Grande do Sul (UFRGS) in Porto Alegre, Brazil.  Master in Audiovisual and Cinema Studies (IMACS) based at the Goethe University.\nField ¬ß\n#filmstudies#filmindustries\nInterests/projects/skills ¬ß\nCinema industries, gender, and data science.\nLinks ¬ß\nhttps://de.linkedin.com/in/isadora-campregher-paiva-b59072124\nhttps://twitter.com/Isa_CamPaiva"},"public_notes/Sculpting-Time-with-Computers/Jake-Berger":{"title":"Jake Berger","links":["tags/archives","tags/digitalpreservation"],"tags":["person","archives","digitalpreservation"],"content":"About ¬ß\nJake is Executive Product Manager of BBC Archive.\nBackground ¬ß\nI‚Äôm not sure!\nJake, if you are reading this and want to add something here, give me a shout.\nField ¬ß\n#archives#digitalpreservation\nInterests/projects/skills ¬ß\nBBC Rewind, Genome, RemArc, BBC SFX\nLinks ¬ß\nhttps://www.bbc.co.uk/archive/stuff-we-do"},"public_notes/Sculpting-Time-with-Computers/Joel-McKim":{"title":"Joel McKim","links":["tags/digitalanimation","tags/architecture","tags/design"],"tags":["person","digitalanimation","architecture","design"],"content":"About ¬ß\nJoel‚Äôs work focuses on the study of digital images and the impact of digital technologies on architecture, art and design. He is the Director of the Vasari Research Centre for Art and Technology and I‚Äôm currently a Visiting Research Fellow at the V&amp;A Research Institutewhere he is working on a project entitled ‚ÄúA Prehistory of Machine Vision: Exploring the V&amp;A Computer Art Collection‚Äù.\nBackground ¬ß\nJoel is Senior Lecturer, Department of Film, Media &amp; Cultural Studies at Birkbeck.  Before that he was a Kenneth P. Dietrich Post-Doctoral Fellow at the University of Pittsburgh in the Department of the History of Art and Architecture and an FQRSC Post-Doctoral Fellow at McGill University in the Department of Art History and Communication Studies, where he participated in the Media and Urban Life research group. He was also a full-time Lecturer in the Department of Communication Studies at Concordia University in Montreal.\nField ¬ß\n#digitalanimation ,#architecture and#design\nInterests/projects/skills ¬ß\nJoel‚Äôs previous professional experience includes web design and internet analytics.\nHe is currently the co-director of the MA Digital Media.\nLinks ¬ß\nJoels‚Äô first book¬†Architecture, Media and Memory: Confronting Complexity in Post-9/11 New York¬†was published last year by Bloomsbury.\nhe is working on a second book entitled¬†Rendered: Digital Animation in Art, Architecture and Design, which draws from an ongoing investigation of the rapidly expanding field of digital animation, undertaken in collaboration with Esther Leslie. This animation project has thus far included the BIH sponsored Life Remade: Politics of Animation Symposium, an ongoing BIMI/Vasari sponsored digital animation screening series, and a 2017 special issue of the journal¬†animation¬†entitled ‚ÄùLife Remade: Critical Animation in the Digital Age.‚Äù"},"public_notes/Sculpting-Time-with-Computers/List-of-all-notes":{"title":"a list of all notes here","links":["public_notes/Sculpting-Time-with-Computers/Participants","public_notes/Sculpting-Time-with-Computers/Sculpting-Time-With-Computers","public_notes/Sculpting-Time-with-Computers/Annotation-guidelines","public_notes/Sculpting-Time-with-Computers/High-dimensional-Cinema","public_notes/Sculpting-Time-with-Computers/List-of-all-notes","public_notes/Sculpting-Time-with-Computers/Reverse-compression-as-motion-estimation","public_notes/Sculpting-Time-with-Computers/cinematic-time","public_notes/Sculpting-Time-with-Computers/collections","public_notes/Sculpting-Time-with-Computers/computational","public_notes/Sculpting-Time-with-Computers/day-one","public_notes/Sculpting-Time-with-Computers/day-two","public_notes/Sculpting-Time-with-Computers/design","public_notes/Sculpting-Time-with-Computers/film","public_notes/Sculpting-Time-with-Computers/high-level-reasoning-about-time","public_notes/Sculpting-Time-with-Computers/historical-time","public_notes/Sculpting-Time-with-Computers/ideas-and-next-steps","public_notes/Sculpting-Time-with-Computers/long-list-of-ideas","public_notes/Sculpting-Time-with-Computers/subjective-time"],"tags":["stc-workshop"],"content":"\nHere is a list of all notes (excluding participants) that together give an account of our activities at the Sculpting Time With Computers workshop.\n\nAnnotation guidelines\nHigh-dimensional Cinema\nList of all notes\nParticipants\nReverse compression as motion estimation\ncinematic time\ncollections\ncomputational\nday one\nday two\ndesign\nfilm\nhigh-level reasoning about time\nhistorical time\nideas and next steps\nlong list of ideas\nsubjective time\n"},"public_notes/Sculpting-Time-with-Computers/Mila-Oiva":{"title":"Mila Oiva","links":["tags/history","tags/soviet","tags/culturalanalytics"],"tags":["person","history","soviet","culturalanalytics"],"content":"About ¬ß\nMila Oiva is a cultural historian enthusiastic about identifying patterns of transnational circulation of knowledge and ideas in a long temporal perspective. Detecting the gaps, overflows, as well as geographical and social differences between the circulating information and perceptions reveal global interconnectednesses and differences. Mila explores circulation of knowledge through case studies focusing on circulation of pseudohistorical contents in Russian language internet in the 2000s and circulation of ideas and footage in Soviet newsreels after World War II. In her research, she analyses text, audio-visual and metadata using computational text, image and network analysis methods in interdisciplinary collaboration with other scholars.\nBackground ¬ß\nMila received her PhD in Cultural History in 2017 from the University of Turku, Finland. She was a visiting Fulbright scholar at the Institute of Slavic, East European, and Eurasian Studies (ISEEES) at UC Berkeley in 2014-2015, and participated in the Culture Analytics long program at the Institute for Pure and Applied Mathematics (IPAM) at UCLA in spring 2016.\nField ¬ß\nDigital#history with a focus on the#soviet era.#culturalanalytics\nInterests/projects/skills ¬ß\n\nSoviet news and newsreels\nCold War era cultural diplomacy\nPolish marketing practices in the Soviet Union\nInterdisciplinary transfer of knowledge in Digital Humanities\n\nLinks ¬ß\nhttps://cudan.tlu.ee/team/mila/"},"public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord":{"title":"Nanne van Noord","links":["tags/machinevision","tags/computervision","tags/culturalanalytics"],"tags":["person","machinevision","computervision","culturalanalytics"],"content":"About ¬ß\nAssistant Professor in the Multimedia Analytics lab of the University of Amsterdam. I‚Äôm interested in improving Multimedia Analysis with and for Visual Culture.\nBefore this I was a Postdoc in CREATE working to improve the Computer Vision capabilties of the CLARIAH infrastructure. This followed my first Postdoc position at the ISIS group in the SEMIAproject. In SEMIA I worked on techniques for exploring AV archives based on perceptual and sensory features.\nBackground ¬ß\nI wrote my PhD thesis at Tilburg University on representation learning for artistic style, the digital version of which can be found here.\n\nField ¬ß\n#machinevision#computervision#culturalanalytics\nInterests/projects/skills ¬ß\n\nComputer vision\nCultural analytics\nVisual representations of style\nInterdisciplinary computational methods\n\nLinks ¬ß\nhttps://nanne.github.io/about/"},"public_notes/Sculpting-Time-with-Computers/Participants":{"title":"Participants","links":["public_notes/Sculpting-Time-with-Computers/Sculpting-Time-With-Computers","public_notes/Sculpting-Time-with-Computers/Andrea-Farina","public_notes/Sculpting-Time-with-Computers/Bel√©n-Vidal","public_notes/Sculpting-Time-with-Computers/Carlo-Bretti","public_notes/Sculpting-Time-with-Computers/Isadora-Campregher","public_notes/Sculpting-Time-with-Computers/Jake-Berger","public_notes/Sculpting-Time-with-Computers/Joel-McKim","public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman","public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie","public_notes/Sculpting-Time-with-Computers/Tom-Brown"],"tags":["stc-workshop"],"content":"\nParticipants of the  Sculpting Time With Computers workshop at KCL, 6-7 July, 2023.\n\nAndrea Farina\nBel√©n Vidal\nCarlo Bretti\nIsadora Campregher\nJake Berger\nJoel McKim\nMila Oiva\nNanne van Noord\nPauline van Mourik Broekman\nStephen McConnachie\nTom Brown\n\nAll information was pulled from publicly available sources online. Please contact me if you want something added, changed, or removed."},"public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman":{"title":"Pauline van Mourik Broekman","links":["tags/art","tags/design","tags/journalism"],"tags":["person","art","design","journalism"],"content":"About ¬ß\nArtist and lecturer at the UAL‚Äôs Creative Computing Institute. Co-founder of Mute magazinethat covered cyberculture, artistic practice, left-wing politics, urban regeneration, biopolitics, direct democracy, net art, the commons, horizontality and UK arts.\nBackground ¬ß\nPhD at the Royal College of Art. Available from: https://researchonline.rca.ac.uk/5281/\nField ¬ß\n#art#design#journalism\nInterests/projects/skills ¬ß\n\nArt, design, and practice-based research\nTacit and embodied knowledge\nOpen education and cultural studies\nSoviet film, twentieth century collaborative art practices, digital culture and economy\n\nLinks ¬ß\nhttps://uk.linkedin.com/in/pauline-van-mourik-broekman-4a85b72\nhttp://www.techne.ac.uk/for-students/techne-students/techne-alumni-list/techne-students-2015/pauline-van-mourik-broekman"},"public_notes/Sculpting-Time-with-Computers/Reverse-compression-as-motion-estimation":{"title":"Reverse compression as motion estimation","links":[],"tags":["stc-workshop"],"content":"\nI wrote this for my PhD. I haven‚Äôt found the actual experiment files, but the method can be recreated from the formulas if there‚Äôs any interest in that. I can probably refine it a little bit if I implement it again.\n"},"public_notes/Sculpting-Time-with-Computers/Sculpting-Time-With-Computers":{"title":"Sculpting Time with Computers","links":["public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman","public_notes/Sculpting-Time-with-Computers/List-of-all-notes","public_notes/Sculpting-Time-with-Computers/day-one","public_notes/Sculpting-Time-with-Computers/day-two","public_notes/Sculpting-Time-with-Computers/Participants"],"tags":["event","stc-workshop","computational-moving-images-machine-vision"],"content":"\nInterdisciplinary Approaches to Computational Moving Images ¬ß\nThis workshop brought together a group of researchers in the fields of digital and computational humanities, computer vision, film, digital preservation and archives, cultural history, and creative computing, to explore together emerging computational approaches to the study of time in moving images.\nIn this small net of notes, I document and summarise our workshop activities (and thanks to Pauline for sharing her comprehensive notes which helped me put everything together in one place). Follow the links below for organic navigation, or find here a list of all notes.\nThe work for this workshop was split over two days:\n\n\nOn day one we explored the modelling of moving images as computational artefacts, and discussed the opportunities and challenges of computational approaches to large collections of moving images.\n\n\nBased on these discussions, on day two we tested some of our ideas in practice using King‚Äôs CREATE HPC cluster, kindly supported by James Graham and Matt Penn from the e-Research teamat KCL.\n\n\n\nParticipants include researchers from leading laboratories in Europe, including the Cultural Data Analytics Open Lab (CUDAN) at Tallinn University and the Cultural Analytics Lab (CANAL) at the University of Amsterdam, as well as archives and digital preservation experts from public UK institutions such as the BBC and the BFI. The workshop was hosted by the Computational Humanities Research Group in the Department of Digital Humanities at King‚Äôs College London. And a big thanks to King‚Äôs Institute for Artificial Intelligence who kindly provided us with welcome packs, including totes, and notebooks."},"public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie":{"title":"Stephen McConnachie","links":["tags/archives","tags/digitalpreservation"],"tags":["person","archives","digitalpreservation"],"content":"About ¬ß\nStephen has worked at the BFI since 2009. In his current role he leads the Data and Digital Preservation department, with strategic and operational responsibility for the BFI National Archive‚Äôs data and digital preservation policies, standards, practices and infrastructure.\nBackground ¬ß\nHe came to the BFI from the television archive ITN Source, where he led the teams of cataloguers, researchers and content editors in the transition from analogue to digital, file-based workflows.\nField ¬ß\n#archives#digitalpreservation\nInterests/projects/skills ¬ß\nStephen‚Äôs team defines and implements documentation and data standards for the BFI‚Äôs collections, creates the BFI Filmography (the national database of British feature films), manages the collections systems (including the Collections Information Database and the Digital Preservation Infrastructure), and delivers data and media to BFI platforms and projects, including BFI Player and the Mediatheque in BFI Southbank.\nLinks ¬ß\nhttps://www2.bfi.org.uk/people/stephen-mcconnachie"},"public_notes/Sculpting-Time-with-Computers/Tom-Brown":{"title":"Tom Brown","links":["tags/filmstudies"],"tags":["person","filmstudies"],"content":"About ¬ß\nTom¬†Brown is Senior Lecturer in Film Studies. He is the author of two monographs: Spectacle in ‚ÄúClassical‚Äù Cinemas: Musicality and Historicity in the 1930s (2016) and Breaking the Fourth Wall: Direct Address in the Cinema (2012). Brown is the co-editor of The Biopic in Contemporary Film Culture (2014), Film Moments: Criticism, History, Theory (2010) and Film and Television After DVD (2008), and the author of numerous other articles and chapters.\nBackground ¬ß\nHe holds a PhD¬†in Film Studies and MA in Film and Television Studies from the University of Warwick and a BA in Film Studies and French from the University of Kent. Prior to working at KCL, he was a Lecturer in Film at the University of Reading.\nField ¬ß\n#filmstudies\nInterests/projects/skills ¬ß\nFilm analysis and film theory, with a focus on:\n\nCinematic ‚Äòclassicism‚Äô, particularly in Hollywood\nThe representation of history on film\nFrench cinema, especially of the 1930s\nCinematic spectacle and film genres (esp. the musical)\nPerformance\n\nLinks ¬ß\nhttps://www.kcl.ac.uk/people/tom-brown"},"public_notes/Sculpting-Time-with-Computers/cinematic-time":{"title":"cinematic time","links":["public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie","public_notes/Sculpting-Time-with-Computers/Reverse-compression-as-motion-estimation","public_notes/Sculpting-Time-with-Computers/Carlo-Bretti","public_notes/Sculpting-Time-with-Computers/day-two"],"tags":["stc-workshop"],"content":"\nThis strand on table 3 focused on cinematic time at the most local level: from shot to shot and even frame to frame; a kind of computational poetics of moving images, which is probably the closest to my own work. We discussed more or less established methods to calculate cutting rates, such as average shot length (see for example cinemetrics and this shorter piece by Stephen Follows), and their automation using techniques such as shot boundary detection.\nOne of the ideas that emerged from these discussions is the subjective experience of film at a more perceptual (phenomenological?) level, and the language we use to describe that kind of experience, e.g. fast or slow. Stephen elaborated on this and got us to ask if there might be a correlation between fast subjects and fast cutting rates, or between editing styles and the perception of time on screen.\nWhat would it take to engineer a fast-slow signal from sequences of frames?\nHow slow is slow cinema?\n\nI mentioned the method we used to calculate the ‚Äúenergy‚Äù section of the Made by Machine BBC programme in 2018, which basically consists on reverse engineering compression. I found some of my writing on the idea and some experiments I ran for my PhD. And Carlo Bretti ran an implementation of shot scale detection during day two of the workshop.\n"},"public_notes/Sculpting-Time-with-Computers/collections":{"title":"collections","links":["public_notes/Sculpting-Time-with-Computers/Jake-Berger","public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie","public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/subjective-time"],"tags":["stc-workshop"],"content":"\nIn this cluster are Jake Berger and Stephen McConnachie, who brought to the group their experience and knowledge working with archives and large collections of moving images, from an institutional perspective in the UK. Joined by Mila Oiva, from a research perspective analysing large collections of Soviet newsreels.\nThese are some of the challenges they highlighted for us in terms of encoding cinematic time for computational analysis:\n\nEncoding performative aspects of carrier technology, for production and exhibition. For example common rules of thumb to estimate the frame rate of a projectionist cranking a film reel by hand in early films (Stephen) or high frame rate such as in some of Ang Lee‚Äôs films.\nWhat‚Äôs an acceptable levels of artifice: e.g. Peter Jackson‚Äôs frame interpolation and colorisation techniques which was very controversial in the archiving community.\nEncoding relational historical categories, such as ‚ÄúVictorian‚Äù or ‚Äúpost-war‚Äù, and non-linear categories, such as cyclical time ‚Äúweekly‚Äù (Mila). This came up again on the discussions on subjective time during the ideation and prototyping sessions.\nJake also made the point of defining use cases for this research, keeping in mind the users and uses of archives, as well as the wider purpose of these public collections. He argued that some of the organisations best positioned to use archives data would be the least likely to be trusted with this data (eg. FAANG‚Äôs).\n"},"public_notes/Sculpting-Time-with-Computers/computational":{"title":"computational","links":["public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/Carlo-Bretti","public_notes/Sculpting-Time-with-Computers/Andrea-Farina","public_notes/Creanalytics","public_notes/Sculpting-Time-with-Computers/collections","public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman","public_notes/Sculpting-Time-with-Computers/Annotation-guidelines"],"tags":["stc-workshop"],"content":"\nIn the computational cluster we had Nanne van Noord, Carlo Bretti, and Andrea Farina. They shared with the workshop the challenges of analysing moving images using units at different scales, e.g. pixels, shots, scenes, films, archives.See for example this table from my article on Creanalytics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtefactProcessing levelExampleCinemaSocial ‚Äì aggregatePopular Hollywood cinemaFilmHumanJurassic Park (1993)ClipHuman/computerRaptors in the Kitchen Scene (https://youtu.be/dnRxQ3dcaQk)ShotHuman-computer130 frames (5.421 s)FrameComputer/humanIndividual frame (512 √ó 340 pixels)PixelsNumeric ‚Äì disaggregateVector ([176800x1]); Tensor ([16, 3, 340, 512])\nThis cluster also highlighted the difficulties of encoding weak conventions or tacit knowledge, which echoes some of the concerns of the collections cluster and the work of Pauline. Later, during the ideation sessions, we came back for example to the idea of detecting ‚Äúscenes‚Äù or ‚Äúblocs‚Äù which are defined by semantic convention and defined annotation guidelines for this kind of unit."},"public_notes/Sculpting-Time-with-Computers/day-one":{"title":"day one","links":["public_notes/Sculpting-Time-with-Computers/film","public_notes/Sculpting-Time-with-Computers/computational","public_notes/Sculpting-Time-with-Computers/collections","public_notes/Sculpting-Time-with-Computers/design","public_notes/Sculpting-Time-with-Computers/Joel-McKim","public_notes/Sculpting-Time-with-Computers/long-list-of-ideas","public_notes/Sculpting-Time-with-Computers/historical-time","public_notes/Sculpting-Time-with-Computers/subjective-time","public_notes/Sculpting-Time-with-Computers/cinematic-time","public_notes/Sculpting-Time-with-Computers/day-two"],"tags":["stc-workshop"],"content":"\n6 July ¬ß\nIntroduction session ¬ß\nAfter informal conversations during coffee, we got started with a general introduction, where I gave an overview of the workshop, including its format, aims and, a provisional definition of cinematic time.\nAs I was organising this workshop, there were two references on the back of my mind:\n\n\nMary Ann Doane‚Äôs excellent book The Emergence of Cinematic Time, in which she makes a case for cinema transforming the very idea of time, and specifically, ‚Äúmodern ideas about continuity and discontinuity, archivability, contingency and determinism, and temporal irreversibility.‚Äù\n\n\nAndrei Tarkovsky‚Äôs Sculpting Time, where he argues for an aesthetics of rhythm: ‚ÄúThe dominant, all-powerful factor of the film image is rhythm, expressing the course of time within the frame.‚Äù\n\n\nI was glad to see that some of these themes emerged during discussion, and I should have made them clearer to participants, so I note them here to make them retrospectively available.\nTo kick things off, we split in four disciplinary clusters: film, computational, collections, and design.  The idea was to allow a disciplinary perspective to come through in this first round of discussions, to later reorganise into inter-disciplinary groups.\n(Thanks to Joel for the action shot below).\n\nIdeation session one: exploration ¬ß\nHaving laid out some of the challenges and opportunities from our cluster perspective, we reorganised into mixed-perspective groups and got to work on a long list of ideas. This session was organised around the principle of fast iteration: collect as many ideas as possible, no matter how far-fetched, to quickly build a large pool of possibilities to work with.\nIdeation session two: refinement ¬ß\nFrom the long list, three ideas organically emerged as the ones participants were most interested in, so we organised these by types of time, one for each table: historical time, subjective time, cinematic time. We remixed again into different working groups, and went on to refine these three ideas by mapping some of the possible inputs and outputs relevant to each kind of time.\nNext, see how we developed this the next day!"},"public_notes/Sculpting-Time-with-Computers/day-two":{"title":"day two","links":["public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/historical-time","public_notes/Sculpting-Time-with-Computers/cinematic-time","public_notes/Creanalytics","public_notes/Sculpting-Time-with-Computers/day-one","public_notes/Sculpting-Time-with-Computers/High-dimensional-Cinema","public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/ideas-and-next-steps","public_notes/Sculpting-Time-with-Computers/Andrea-Farina","public_notes/Sculpting-Time-with-Computers/Annotation-guidelines"],"tags":["stc-workshop"],"content":"\n7 July ¬ß\nFast prototyping session ¬ß\nOn day two, we gathered again to decide what ideas we could put to the test and fast-prototype. We examined two datasets: Mila‚Äôs soviet newsreels and this publicly available sample from BBC Archives. We decided to focus on the newsreels, which have richer metadata. The idea was to test two methods that map to our discussions of historical time and meaningful objects, and cinematic time and fast/slow editing. We tested on the newsreels:\n\nOpen clip ‚Äî&gt; an open source implementation of CLIP pretrained on the LAION 2B dataset. The core idea behind CLIP is to have text and images in a shared representational space that allows calculations of proximity between them. This can then be used for analysis and retrieval.\n\n\n\nCinescale ‚Äî&gt; a shot scale classification model trained on a set of manually annotated frames from 124 films. The goal is to classify different shots by their scale: close-up, medium shot, long shot, etc. In this paper I tried shot classification without annotations using face detection, but that approach is of course limited to faces, whereas the Cinescale library can in principle pick up close-ups of objects for example.\n\n\nWe know these techniques have limitations, as explored during day one of the workshop. But we also think that, combined, these methods complement each other and can be useful for the analysis of temporal dynamics in large collections of moving images, for example as a way to detect editing patterns and co-relate them to themes and known historical time frames.\nTesting this combination against the newsreel data set is helpful because thanks to Mila‚Äôs sustained work on these reels, we know something about their local as well as global structures, so we can apply the methods to the collection of images and use the collection of images as a way to test the the methods. This approach reminded me of the notion of ‚Äùpredicting the past‚Äù by the wonderful Tobias Blanke.\nDuring the panel on high-dimensional cinema, I referred to this back and forth between modalities of inquiry as building ‚Äúaesthetically sensitive systems‚Äù, and in our discussion the day after Nanne suggested ‚Äúaesthetically sensitive retrieval‚Äù as an applied version of this idea. We wrapped up the day with some ideas and next steps along these lines.\nMila and Andrea also sampled the news reels and produced an annotation guidelines document for possible future human annotation, and that directly informs editing style detection and open CLIP queries.\n\nThanks to James Graham and Matt Penn from the e-research team at KCL, who helped us setup ad-hoc VMs to  run these tests in the CREATE HPC cluster from the Digital Humanities Computer Lab (pictured above)."},"public_notes/Sculpting-Time-with-Computers/design":{"title":"design","links":["public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman","public_notes/Sculpting-Time-with-Computers/Joel-McKim","public_notes/Sculpting-Time-with-Computers/film","public_notes/Sculpting-Time-with-Computers/collections"],"tags":["stc-workshop"],"content":"\nThe design cluster included Pauline van Mourik Broekman, Joel McKim, and myself. We discussed some of the interfaces used to access and manipulate time in audiovisual media, for example the ubiquitous timeline in editing software. We discussed how interfaces structure the perception of viewing time, for instance in the default ‚Äúenforced flow‚Äù of streaming platforms in which users have to explicitly tell the system to stop. Or the programmed deskilling of users through the constant reconfiguration of systems and interfaces.\nEchoing the film and collections clusters, we also discussed some of the embodied, performative, and economic aspects in the production of films, specifically pre and post production practices. Pauline referenced Wendy Apple‚Äôs documentary The Cutting Edge (not the 1992 movie). Joel also mentioned how technical challenges in moving around data in space also shape the perceived temporality of media, he gave Sohonet as an example."},"public_notes/Sculpting-Time-with-Computers/film":{"title":"film","links":["public_notes/Sculpting-Time-with-Computers/Bel√©n-Vidal","public_notes/Sculpting-Time-with-Computers/Tom-Brown","public_notes/Sculpting-Time-with-Computers/Isadora-Campregher","public_notes/Sculpting-Time-with-Computers/high-level-reasoning-about-time","public_notes/Sculpting-Time-with-Computers/subjective-time","public_notes/Sculpting-Time-with-Computers/computational"],"tags":["stc-workshop"],"content":"\nIn the film cluster we had Bel√©n Vidal, Tom Brown, and Isadora Campregher.\nThey brought our attention issues of embodied spectatorship, including the difficulties of encoding  high-level reasoning about time and subjective time, which are needed to understand complex representations such as age in film. These levels were seen as mostly inaccessible to existing computational methods, and easily mobilised by human viewers.\nIt was interesting to see some of these limitations having correspondences in the computational cluster discussion about units of analysis."},"public_notes/Sculpting-Time-with-Computers/high-level-reasoning-about-time":{"title":"high-level reasoning about time","links":["public_notes/Sculpting-Time-with-Computers/film","public_notes/Sculpting-Time-with-Computers/cinematic-time","public_notes/Sculpting-Time-with-Computers/historical-time","public_notes/Sculpting-Time-with-Computers/subjective-time"],"tags":["stc-workshop"],"content":"\nThe film cluster contributed by outlining the diversity of approaches and levels of analysis about time in moving images. Some of these include recognising and manipulating several representations of time in a global working space and using long-term memory, namely:\n\nmotion time\ncharacter time\nnarrative time\nfilm time\nlived time\nhistorical time\n\nCharacter time and lived time, for example, are closely linked to the articulation of  subjective time, also discussed during the ideation sessions.\nComputational models were recognised to be very limited in their capacity to simulate or perform this kind of reasoning. However, during the workshop we considered the opportunities of some of these methods to contribute to our understanding of some of these higher-level representations."},"public_notes/Sculpting-Time-with-Computers/historical-time":{"title":"historical time","links":["public_notes/Sculpting-Time-with-Computers/Jake-Berger","public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie","public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/day-two"],"tags":["stc-workshop"],"content":"\nThis table was concerned with epochs or periods of time commonly used to describe, classify and retrieve moving images, using terms such as ‚ÄúVictorian‚Äù, ‚Äúpost-war‚Äù, ‚Äúpre-modern‚Äù, etc. Jake  and Stephen mentioned this is often how archives are searched by users and for commercial licencing: ‚ÄúI need a shot of a street in a rainy night in Victorian London‚Äù.  They agreed an ‚Äúepoch‚Äù or ‚Äúera‚Äù detector would be very helpful for archive users.\nWhat would it take to design an ‚Äúera detector‚Äù?\nWe discussed possible approaches to infer this kind of represented historical time, including using detected objects as proxies for time periods, e.g. mobile phones, top hats, pylons, etc. This was provisionally called the meaningful objects approach, and the idea was to detect the first occurrence of these objects in large audiovisual archives. Nanne  noted that while this is possible, object detection is usually trained on contemporary objects, so there is an additional layer of modification needed.\n\nWe tried for example to query ‚ÄúVictorian‚Äù in the frozen in time video search implementation, which outputs mostly buildings. One of the most revealing (and funny) moments was when we queried for ‚ÄúVictorian person‚Äù:\n\nStill, the intuition is that something about depicted eras can be captured by this type of system stayed with us and was put to the test during day two."},"public_notes/Sculpting-Time-with-Computers/ideas-and-next-steps":{"title":"ideas and next steps","links":["public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord","public_notes/Sculpting-Time-with-Computers/collections"],"tags":["stc-workshop"],"content":"\nThree main ideas were discussed to follow up on the work from this workshop, more or less in order of immediacy and scale:\n\nSubmit a joint paper to the CUDAN Cultural Analytics Conference in Tallinn, in December. Mila, Nanne, and myself, are preparing an abstract based on the idea of ‚Äúaesthetically sensitive methods‚Äù and the promising findings of the tests we ran on the collection of soviet newsreels during the second day.\nFind ways to formalise the research network to strengthen our collaboration. The idea is to hold these meetings and workshops more regularly, allowing us to block some time for joint discussion and experimentation.\nExplore if there is an appetite to develop a larger project proposal in partnership with one of the collections.\n"},"public_notes/Sculpting-Time-with-Computers/long-list-of-ideas":{"title":"long list of ideas","links":[],"tags":["stc-workshop"],"content":"\nWe split into three working groups mixing from our initial disciplinary clusters. The goal was to come up with as many ideas as possible for a system/tool/method that we thought would be valuable for investigating cinematic time. Pictures of the long lists below.\nLong list table 1 ¬ß\n\nLong list table 2 ¬ß\n\nLong list table 3 ¬ß\n"},"public_notes/Sculpting-Time-with-Computers/subjective-time":{"title":"subjective time","links":["public_notes/Sculpting-Time-with-Computers/Bel√©n-Vidal","public_notes/Sculpting-Time-with-Computers/historical-time","public_notes/Sculpting-Time-with-Computers/cinematic-time","public_notes/Sculpting-Time-with-Computers/Jake-Berger","public_notes/Sculpting-Time-with-Computers/Andrea-Farina","public_notes/Sculpting-Time-with-Computers/Mila-Oiva","public_notes/Sculpting-Time-with-Computers/Isadora-Campregher","public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie"],"tags":["stc-workshop"],"content":"\nTime not only as duration, but as perceived through subjective experience, by audiences and by characters on screen. Bel√©n pointed our attention to the relation between time as duration, e.g. a performer‚Äôs age in years, and subjective time, e.g. a character‚Äôs age on screen. This was later discussed as one strand of encoding time using computational methods in the first and second ideation sessions, and alongside historical time and cinematic time.\nFor the refinement session, we considered possible methods to analyse age and ageing on screen, including character age, performer age, and the gap in between. Here is how this table looked during this refinement stage:\n\nAs a starting point, we considered what it would take to get the difference between performer age and performed age, split by gender. Performer age as a duration can be calculated by identifying the date of birth of performer, e.g. Sean Connery (1930), and their approximate age at a given performance, e.g. Diamonds are Forever (1971), which means he was ~41 at the time of filming.\nPerformed age is much more difficult. We discussed possible methods for performed age detection, including extracted features from faces, speech and even gestures ‚Äïall of which are we thought tended to be noisy/inaccurate/biased. We also considered text sources such as synopses, where main characters are sometimes given age descriptors such as ‚Äúten-year-old Finnegan‚Äù or relational such as ‚Äúher mother Estella‚Äù. Other sources include scripts, which can be more accurate but as Jake reminded us are also less publicly available, and in some cases also the original literary sources of film adaptations (I think it was Andrea who mentioned this).\nIn terms of gender, Mila and Isadora referred to their experience with gender detection by name, and Stephen was in touch later to contribute the INA speech segmenter with gender identification, and his project on gender inference by first name."},"public_notes/The-Digital-Pastoral":{"title":"The Digital Pastoral","links":["tags/stablediffusion","public_notes/Transmediale-2023"],"tags":["stable-diffusion","text-to-image","ai-art","stablediffusion"],"content":"\nI created the image below using#stablediffusion (before it was cool and every media scholar started doing it) and used it for my very short presentation at Transmediale 2023.\n\nI can‚Äôt recall the exact prompt, but I do remember I referenced the illustrations on Jakub R√≥≈ºalski whom I discovered through Scythe.\nAnd here‚Äôs a link to the newspaper!"},"public_notes/Tools/Mermaid-diagrams":{"title":"Mermaid diagrams","links":["tags/criticaltechnicalpractice","private/Projects/Large/VANGE"],"tags":["tools","diagrams","documentation","criticaltechnicalpractice"],"content":"\nJavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.\n\nIt has a live editor for non-programmers, and has some tutorials. Maybe relevant for students. There‚Äôs a Jupyter integration too. Interesting for#criticaltechnicalpractice\nMaybe I can use this to render an maintain a web version of the VANGE diagram in my office.\n"},"public_notes/Tools/Motion-Canvas":{"title":"Motion Canvas","links":["tags/criticaltechnicalpractice","tags/foss"],"tags":["tools","animation","diagramas","documentation","criticaltechnicalpractice","foss"],"content":"\n\nVisualize Complex Ideas Programmatically.\nIt‚Äôs a specialized tool designed to create informative vector animations and synchronize them with voice-overs. It‚Äôs not meant to be a replacement for traditional video editing software.\n\nProcedural animation tool, that can be integrated with a web-editor to sync sound and produce animated videos. Great for education, documentation, and reflection in#criticaltechnicalpractice Reminds me a bit of Flash action script, but#foss and nicer for rapid development and to comunicate abstract ideas precisely. Think of Freya Holm√©r videos or Ms Coffee Beans.\n"},"public_notes/Tools/Movie-Barcodes":{"title":"Movie Barcodes","links":["public_notes/Video-as-data","public_notes/Cinema-and-Machine-Vision"],"tags":["tools"],"content":"\nRecently I‚Äôve been revisiting ‚Äúmovie bar codes‚Äù like this one, from Made by Machine:\n\nThese images are in effect data visualisations that render a whole film as a single image. The process of making them involves sampling frames from a video file, and stretching them as lines in chronological order to visualise the colour-to-time dynamics of the underlying moving images. The idea has been around for a while, and there are now on-demand printing services that sell poster of movie bar codes online.\nMovie bar codes can be created with relatively simple processing and manipulation of Video as data, with no machine learning involved.  They are nice to look at but also tell something about the films, and I think they can be used as a pre or post processing technique in combination with other more advanced methods for computational analysis of moving images.\nHere are some tools I‚Äôve been using to make movie bar codes:\n\nMovie Barcodes ‚Äî&gt; back end: FFMPEG + PIL\nMovie Barcode ‚Äî&gt; wrapped as a Python library. Update: only runs on Python 3.10\nmovie-barcodes ‚Äî&gt; command line application, back end is OpenCV and numpy\n\nI‚Äôve been trying these out to make an image for the cover of my book, Cinema and Machine Vision, possibly warped as as a circle, similar to this example (in R). The polar transformation seems rather onerous to implement directly in Python (for me anyway), but a similar effect can be achieved using the polar filter in GIMP, or the ImageMagick‚Äôs polar distortion transformation. I‚Äôd like to do a mini tutorial on this for my visualisation students, but here‚Äôs a first preview:\n"},"public_notes/Tools/Streaming-Video-Model":{"title":"Streaming Video Model","links":[],"tags":["machine-vision"],"content":"\nInteresting video model that accounts for frames and sequences in a unified model.\nAbstract:\n\n\nVideo understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at this https URL.\n"},"public_notes/Tools/Videogrep":{"title":"Videogrep","links":[],"tags":["supercut","montage","tools","words-and-images"],"content":"\n\nVideogrep is a command line tool that searches through dialog in video files and makes supercuts based on what it finds. It will recognize .srt or .vtt subtitle tracks, or transcriptions that can be generated with vosk, pocketsphinx, and other tools.\n\nSam posted an implementation of Videogrep that runs on Colab, which made me think of running this on a Jupyter Lab instance from our College‚Äôs computer cluster, and then possibly using it for teaching. The college does not host its own notebook instance, as far as I know, but the cluster can serve a headless Jupyter that can be ‚Äútunnelled in‚Äù through ssh.\nHere‚Äôs a supercut of recent lecture by  Shannon Mattern at KCL called ‚ÄúModeling Doubt, Coding Humility: A Speculative Syllabus‚Äù\n\nInterestingly, not one single mention of ‚Äúcode‚Äù or ‚Äúcoding‚Äù, nor of ‚Äúhumility‚Äù. I recommend watching the whole talk, though.\nHere‚Äôs the repositoryfor Videogrep. Here‚Äôs the Colab minimal example.\nAnd here are other examples from Sam‚Äôs repo:\n\nsilence extraction\nautomatically creating supercuts\ncreating supercuts based on youtube searches\ncreating supercuts from specific parts of speech\ncreating supercuts from spacy pattern matching\n"},"public_notes/Tools/Zeeschuimer":{"title":"Zeeschuimer","links":[],"tags":["4cat","social-media-analysis","ux-methods","teaching"],"content":"\n\nZeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and collects data about the items you see in a platform‚Äôs web interface for later systematic analysis. Its target audience is researchers who wish to systematically study content on social media platforms that resist conventional scraping or API-based data collection.\n\n\nThis would definitely be of interest to some of my UX students as a way to integrate in their usability tests and cultural probes when analysing social media usage.\nRepository here."},"public_notes/Transmediale-2023":{"title":"Transmediale 2023","links":[],"tags":["events","publication"],"content":"\nI attended Transmediale and gave a very small presentation as part of the panel Toward Minor Tech.\n\nThis is the result of a workshop organised by by Geoff Cox and Christian Ulrik Andersen every year as part of SHAPE Digital Citizenship &amp; Digital Aesthetics Research Center, Aarhus University and Centre for the Study of the Networked Image, London South Bank University, this year in collaboration with King‚Äôs College London.\nThe workshop took place over several days in London and inlcuded public talks from Marloes de Valk on the damaged earth catalog and from Tung-Hui Hu on digital lethargy.\nMy contribution to the publication was titled: A minor critique of minor tech and can be found as a PDF, along with all other contirbutions, by clicking here.\n"},"public_notes/Video-as-data":{"title":"Video as data in the US","links":[],"tags":["event","computational-moving-images","machine-vision"],"content":"\nThis looks interesting!\nSee twitter post here.\n"},"public_notes/Video-understanding-community":{"title":"Video-understanding community","links":["public_notes/Creanalytics","public_notes/Video-as-data"],"tags":["video","machine-vision"],"content":"\nSome interesting recent work on video understanding tasks and practitioners from computer science.\nCMD Challenge\nFrom the VGG group at Oxford. It uses a version of the condensed movie datasetchallenge, based on the same Movieclip collection I describe in Creanalytics. See their repo for challenge details, and the description below:\n\nFocus: The focus of this challenge is on the long-range understanding of high-level narrative structure in movies.\n\n\nOverview: In the challenge, participants are invited to build a system to retrieve 2-3 minute video clips from movies using corresponding high-level natural language descriptions and a wide range of pre-computed visual features from several pre-trained expert models. Each 2-3 minute clip constitutes a key scene from a movie, each representing important parts in the storyline. Each clip is accompanied by a high-level semantic description which describes the storyline. This includes the motivations of the characters, actions, scenes, objects, interactions and relationships. Participants will use a new challenge version of the Condensed Movies Dataset (CMD) for both training and testing of their retrieval systems.\n\nThis group/challenge is related to the Video as data ICA workshop. And also to a broader push to integrate natural language and visual understandings. See for example the work of Max Bain, Lisa Anne Hendricks at Deepmind and Andrew Brown, ex VGG now at Meta. See below:\nhttps://www.youtube.com/watch?v=GzIphByhXDc"},"public_notes/export/EINA-conference/plugin/chalkboard/README":{"title":"README","links":[],"tags":[],"content":"Chalkboard ¬ß\nWith this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n\nyou can make notes directly on the slides, e.g. to comment on certain aspects,\nyou can open a chalkboard or whiteboard on which you can make notes.\n\nThe main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation ¬ß\nCopy the file plugin.js and the  img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n&lt;script src=&quot;plugin/chalkboard/plugin.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;plugin/chalkboard/customcontrols.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealChalkboard, RevealCustomControls ],\n        // ...\n    });\n&lt;/script&gt;\nThe following stylesheet\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/chalkboard/style.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/customcontrols/style.css&quot;&gt;\nhas to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css&quot;&gt;\n\nto the head section of you HTML-file.\nUsage ¬ß\nMouse or touch ¬ß\n\nClick on the pen symbols at the bottom left to toggle the notes canvas or chalkboard\nClick on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active)\nClick on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard)\nClick the left mouse button and drag to write on notes canvas or chalkboard\nClick the right mouse button and drag to wipe away previous drawings\nTouch and move to write on notes canvas or chalkboard\nTouch and hold for half a second, then move to wipe away previous drawings\n\nKeyboard ¬ß\n\nPress the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings\nPress the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard\nPress the ‚Äòc‚Äô key to toggle the notes canvas\nPress the ‚Äòb‚Äô key to toggle the chalkboard\nPress the ‚Äòd‚Äô key to download drawings\nPress the ‚Äòx‚Äô key to cycle colors forward\nPress the ‚Äòy‚Äô key to cycle colors backward\n\nPlayback ¬ß\nIf the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing ¬ß\nThe plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export ¬ß\nIf the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration ¬ß\nThe plugin has several configuration options:\n\nboardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines.\nchalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines.\nchalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0.\nstorage: Optional variable name for session storage of drawings.\nsrc: Optional filename for pre-recorded drawings.\nreadOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function).\ntransition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed.\ntheme: Can be set to either &quot;chalkboard&quot; or &quot;whiteboard&quot;.\n\nThe following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n\nbackground: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background image for the chalkboard.\ngrid: By default whiteboard and chalkboard themes include a grid pattern on the background. This pattern can be modified by setting the color, the distance between lines, and the line width, e.g. { color: &#039;rgb(127,127,255,0.1)&#039;, distance: 40, width: 2}. Alternatively, the grid can be removed by setting the value to false.\neraser: An image path and radius for the eraser.\nboardmarkers: A list of boardmarkers with given color and cursor.\nchalks: A list of chalks with given color and cursor.\nrememberColor: Whether to remember the last selected color for the slide canvas or the board.\n\nAll of the configurations are optional and the default values shown below are used if the options are not provided.\nReveal.initialize({\n\t// ...\n    chalkboard: {\n        boardmarkerWidth: 3,\n        chalkWidth: 7,\n        chalkEffect: 1.0,\n        storage: null,\n        src: null,\n        readOnly: undefined,\n        transition: 800,\n        theme: &quot;chalkboard&quot;,\n        background: [ &#039;rgba(127,127,127,.1)&#039; , path + &#039;img/blackboard.png&#039; ],\n        grid: { color: &#039;rgb(50,50,10,0.5)&#039;, distance: 80, width: 2},\n        eraser: { src: path + &#039;img/sponge.png&#039;, radius: 20},\n        boardmarkers : [\n                { color: &#039;rgba(100,100,100,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-black.png), auto&#039;},\n                { color: &#039;rgba(30,144,255, 1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-blue.png), auto&#039;},\n                { color: &#039;rgba(220,20,60,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-red.png), auto&#039;},\n                { color: &#039;rgba(50,205,50,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-green.png), auto&#039;},\n                { color: &#039;rgba(255,140,0,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-orange.png), auto&#039;},\n                { color: &#039;rgba(150,0,20150,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-purple.png), auto&#039;},\n                { color: &#039;rgba(255,220,0,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-yellow.png), auto&#039;}\n        ],\n        chalks: [\n                { color: &#039;rgba(255,255,255,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-white.png), auto&#039;},\n                { color: &#039;rgba(96, 154, 244, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-blue.png), auto&#039;},\n                { color: &#039;rgba(237, 20, 28, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-red.png), auto&#039;},\n                { color: &#039;rgba(20, 237, 28, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-green.png), auto&#039;},\n                { color: &#039;rgba(220, 133, 41, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-orange.png), auto&#039;},\n                { color: &#039;rgba(220,0,220,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-purple.png), auto&#039;},\n                { color: &#039;rgba(255,220,0,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-yellow.png), auto&#039;}\n        ]\n    },\n    customcontrols: {\n  \t\tcontrols: [\n  \t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen-square&quot;&gt;&lt;/i&gt;&#039;,\n  \t\t\t  title: &#039;Toggle chalkboard (B)&#039;,\n  \t\t\t  action: &#039;RevealChalkboard.toggleChalkboard();&#039;\n  \t\t\t},\n  \t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen&quot;&gt;&lt;/i&gt;&#039;,\n  \t\t\t  title: &#039;Toggle notes canvas (C)&#039;,\n  \t\t\t  action: &#039;RevealChalkboard.toggleNotesCanvas();&#039;\n  \t\t\t}\n  \t\t]\n    },\n    // ...\n \n});\nLicense ¬ß\nMIT licensed\nCopyright (C) 2021 Asvin Goel"},"public_notes/export/EINA-conference/plugin/chart/README":{"title":"README","links":[],"tags":[],"content":"Chart ¬ß\nA plugin for Reveal.js allowing to easily add charts using Chart.js.\nCheck out the live demo\nInstallation ¬ß\nCopy the file plugin.js into the plugin folder of your reveal.js presentation, i.e. plugin/chart.\nAdd the plugin and Chart.js to the dependencies in your presentation, as below.\n&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.2.0/chart.min.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;plugin/chart/plugin.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealChart ],\n        // ...\n    });\n&lt;/script&gt;\nConfiguration ¬ß\nThe plugin has several parameters that you can set for your presentation by providing an chart option in the reveal.js initialization options. Note that all configuration parameters are optional and the defaults of Chart.js will be used for parameters that are not specified.\nReveal.initialize({\n\t// ...\n\tchart: {\n\t\tdefaults: {\n\t\t\tcolor: &#039;lightgray&#039;, // color of labels\n\t\t\tscale: {\n\t\t\t\tbeginAtZero: true,\n\t\t\t\tticks: { stepSize: 1 },\n\t\t\t\tgrid: { color: &quot;lightgray&quot; } , // color of grid lines\n\t\t\t},\n\t\t},\n\t\tline: { borderColor: [ &quot;rgba(20,220,220,.8)&quot; , &quot;rgba(220,120,120,.8)&quot;, &quot;rgba(20,120,220,.8)&quot; ], &quot;borderDash&quot;: [ [5,10], [0,0] ] },\n\t\tbar: { backgroundColor: [ &quot;rgba(20,220,220,.8)&quot; , &quot;rgba(220,120,120,.8)&quot;, &quot;rgba(20,120,220,.8)&quot; ]},\n\t\tpie: { backgroundColor: [ [&quot;rgba(0,0,0,.8)&quot; , &quot;rgba(220,20,20,.8)&quot;, &quot;rgba(20,220,20,.8)&quot;, &quot;rgba(220,220,20,.8)&quot;, &quot;rgba(20,20,220,.8)&quot;] ]},\n\t},\n        // ...\n});\nThe defaults parameter  will overwrite Chart.defaults. Furthermore, for any chart type, e.g. line, bar, etc., the parameters for the individual datasets can be specified. Where Chart.js allows to specify a single parameter for a particular dataset, the plugin allows to specify an array of values for this parameter, which will automatically be assigned to the different datasets. Note that if there are more datasets than elements in the array, the plugin will start again with the first value in the array.\nUsage ¬ß\nA chart can be included in a slide by adding a canvas element with the data-chart attribute set to the desired chart type.\nThe chart can be configured within the canvas body by a JSON string embedded into an HTML comment.\n&lt;canvas data-chart=&quot;line&quot; &gt;\n&lt;!--\n{\n &quot;data&quot;: {\n  &quot;labels&quot;: [&quot;January&quot;,&quot; February&quot;,&quot; March&quot;,&quot; April&quot;,&quot; May&quot;,&quot; June&quot;,&quot; July&quot;],\n  &quot;datasets&quot;:[\n   {\n    &quot;data&quot;:[65,59,80,81,56,55,40],\n    &quot;label&quot;:&quot;My first dataset&quot;,&quot;backgroundColor&quot;:&quot;rgba(20,220,220,.8)&quot;\n   },\n   {\n    &quot;data&quot;:[28,48,40,19,86,27,90],\n    &quot;label&quot;:&quot;My second dataset&quot;,&quot;backgroundColor&quot;:&quot;rgba(220,120,120,.8)&quot;\n   }\n  ]\n }\n}\n--&gt;\n&lt;/canvas&gt;\nIt is possible to provide the chart data by comma separated values and use the JSON string within the HTML comment to configure the chart layout.\n&lt;canvas class=&quot;stretch&quot; data-chart=&quot;line&quot;&gt;\nMy first dataset, 65, 59, 80, 81, 56, 55, 40\n&lt;!-- This is a comment that will be ignored --&gt;\nMy second dataset, 28, 48, 40, 19, 86, 27, 90\n&lt;!--\n{\n &quot;data&quot; : {\n  &quot;labels&quot; : [&quot;Enero&quot;, &quot;Febrero&quot;, &quot;Marzo&quot;, &quot;Avril&quot;, &quot;Mayo&quot;, &quot;Junio&quot;, &quot;Julio&quot;],\n  &quot;datasets&quot; : [{ &quot;borderColor&quot;: &quot;#0f0&quot;, &quot;borderDash&quot;: [&quot;5&quot;,&quot;10&quot;] }, { &quot;borderColor&quot;: &quot;#0ff&quot; } ]\n }\n}\n--&gt;\n&lt;/canvas&gt;\nThe layout configuration provided in chart parameter (see Configuration) will be used by default and only those parameters that are specified in a JSON string are used to overwrite the default values. If no JSON string is provided to configure the chart layout the default configuration is used. Note, that if no labels for the data points are provided by a JSON string, the plugin expects that the first row provides table headers.\n&lt;canvas data-chart=&quot;line&quot;&gt;\nMonth, January, February, March, April, May, June, July\nMy first dataset, 65, 59, 80, 81, 56, 55, 40\nMy second dataset, 28, 48, 40, 19, 86, 27, 90\n&lt;/canvas&gt;\nThe chart data can also be provided in an external CSV file. To include external data, the filename must be specified using the data-chart-src attribute of the canvas element. The CSV file is expected to only contain data values, whereas options for drawing the chart can be given as described above.\n&lt;canvas data-chart=&quot;bar&quot; data-chart-src=&quot;chart/data.csv&quot;&gt;\n&lt;!--\n{\n&quot;data&quot; : {\n&quot;datasets&quot; : [{ &quot;backgroundColor&quot;: &quot;#0f0&quot; }, { &quot;backgroundColor&quot;: &quot;#0ff&quot; } ]\n},\n&quot;options&quot;: { &quot;scales&quot;: { &quot;x&quot;: { &quot;stacked&quot;: true }, &quot;y&quot;: { &quot;stacked&quot;: true } } }\n}\n--&gt;\n&lt;/canvas&gt;\nLicense ¬ß\nMIT licensed\nCopyright (C) 2021 Asvin Goel"},"public_notes/export/EINA-conference/plugin/customcontrols/README":{"title":"README","links":[],"tags":[],"content":"Custom controls ¬ß\nThis plugin allows to add responsive custom controls to reveal.js which allow arbitrary positioning, layout, and behaviour of the controls.\nCheck out the live demo\nInstallation ¬ß\nCopy the files plugin.js and style.css into the plugin folder of your reveal.js presentation, i.e. plugin/customcontrols and load the plugin as shown below.\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/customcontrols/style.css&quot;&gt;\n&lt;script src=&quot;plugin/customcontrols/plugin.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealCustomControls ],\n        // ...\n    });\n&lt;/script&gt;\nNote, without configuration you need to add\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css&quot;&gt;\nbetween &lt;head&gt; and &lt;/head&gt; of your HTML file because the defaults use Font Awesome.\nConfiguration ¬ß\nThe plugin can be configured by adding custom controls and changing the layout of the slide number, e.g., by:\nReveal.initialize({\n\t// ...\n  customcontrols: {\n\t\tcontrols: [\n      {\n\t\t\t  id: &#039;toggle-overview&#039;,\n\t\t\t  title: &#039;Toggle overview (O)&#039;,\n\t\t\t  icon: &#039;&lt;i class=&quot;fa fa-th&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  action: &#039;Reveal.toggleOverview();&#039;\n\t\t\t},\n\t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen-square&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  title: &#039;Toggle chalkboard (B)&#039;,\n\t\t\t  action: &#039;RevealChalkboard.toggleChalkboard();&#039;\n\t\t\t},\n\t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  title: &#039;Toggle notes canvas (C)&#039;,\n\t\t\t  action: &#039;RevealChalkboard.toggleNotesCanvas();&#039;\n\t\t\t}\n\t\t]\n\t},\n\t// ...\n \n});\nThe id and title are optional. The configuration should be self explaining and any number of controls can be added. The style file can be altered to control the layout and responsiveness of the custom controls.\nLicense ¬ß\nMIT licensed\nCopyright (C) 2020 Asvin Goel"},"public_notes/export/EINA-conference/plugin/menu/CONTRIBUTING":{"title":"CONTRIBUTING","links":[],"tags":[],"content":"Contributing ¬ß\nBug Reports ¬ß\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\nPull Requests ¬ß\n\nShould follow the coding style of the file you work in\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n"},"public_notes/export/EINA-conference/plugin/menu/README":{"title":"README","links":[],"tags":[],"content":"reveal.js-menu ¬ß\nA slideout menu plugin for Reveal.js to quickly jump to any slide by title. Also optionally change the theme and set the default transition. Check out the live demo\nInstallation ¬ß\nBower ¬ß\nDownload and install the package in your project:\nbower install reveal.js-menu\nAdd the plugin to your presentation, as below.\n&lt;script src=&quot;bower_components/reveal.js-menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nnpm ¬ß\nDownload and install the package in your project:\nnpm install --save reveal.js-menu\nAdd the plugin to your presentation, as below.\n&lt;script src=&quot;node_modules/reveal.js-menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nManual ¬ß\nCopy this repository into the plugins folder of your reveal.js presentation, ie plugins/menu.\nAdd the plugin to the dependencies in your presentation, as below.\n&lt;script src=&quot;plugin/menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nConfiguration ¬ß\nYou can configure the menu for your presentation by providing a menu option in the reveal.js initialization options. Note that all config values are optional and will default as specified below.\nReveal.initialize({\n  // ...\n \n  menu: {\n    // Specifies which side of the presentation the menu will\n    // be shown. Use &#039;left&#039; or &#039;right&#039;.\n    side: &#039;left&#039;,\n \n    // Specifies the width of the menu.\n    // Can be one of the following:\n    // &#039;normal&#039;, &#039;wide&#039;, &#039;third&#039;, &#039;half&#039;, &#039;full&#039;, or\n    // any valid css length value\n    width: &#039;normal&#039;,\n \n    // Add slide numbers to the titles in the slide list.\n    // Use &#039;true&#039; or format string (same as reveal.js slide numbers)\n    numbers: false,\n \n    // Specifies which slide elements will be used for generating\n    // the slide titles in the menu. The default selects the first\n    // heading element found in the slide, but you can specify any\n    // valid css selector and the text from the first matching\n    // element will be used.\n    // Note: that a section data-menu-title attribute or an element\n    // with a menu-title class will take precedence over this option\n    titleSelector: &#039;h1, h2, h3, h4, h5, h6&#039;,\n \n    // If slides do not have a matching title, attempt to use the\n    // start of the text content as the title instead\n    useTextContentForMissingTitles: false,\n \n    // Hide slides from the menu that do not have a title.\n    // Set to &#039;true&#039; to only list slides with titles.\n    hideMissingTitles: false,\n \n    // Adds markers to the slide titles to indicate the\n    // progress through the presentation. Set to &#039;false&#039;\n    // to hide the markers.\n    markers: true,\n \n    // Specify custom panels to be included in the menu, by\n    // providing an array of objects with &#039;title&#039;, &#039;icon&#039;\n    // properties, and either a &#039;src&#039; or &#039;content&#039; property.\n    custom: false,\n \n    // Specifies the themes that will be available in the themes\n    // menu panel. Set to &#039;true&#039; to show the themes menu panel\n    // with the default themes list. Alternatively, provide an\n    // array to specify the themes to make available in the\n    // themes menu panel, for example...\n    //\n    // [\n    //     { name: &#039;Black&#039;, theme: &#039;dist/theme/black.css&#039; },\n    //     { name: &#039;White&#039;, theme: &#039;dist/theme/white.css&#039; },\n    //     { name: &#039;League&#039;, theme: &#039;dist/theme/league.css&#039; },\n    //     {\n    //       name: &#039;Dark&#039;,\n    //       theme: &#039;lib/reveal.js/dist/theme/black.css&#039;,\n    //       highlightTheme: &#039;lib/reveal.js/plugin/highlight/monokai.css&#039;\n    //     },\n    //     {\n    //       name: &#039;Code: Zenburn&#039;,\n    //       highlightTheme: &#039;lib/reveal.js/plugin/highlight/zenburn.css&#039;\n    //     }\n    // ]\n    //\n    // Note: specifying highlightTheme without a theme will\n    // change the code highlight theme while leaving the\n    // presentation theme unchanged.\n    themes: false,\n \n    // Specifies the path to the default theme files. If your\n    // presentation uses a different path to the standard reveal\n    // layout then you need to provide this option, but only\n    // when &#039;themes&#039; is set to &#039;true&#039;. If you provide your own\n    // list of themes or &#039;themes&#039; is set to &#039;false&#039; the\n    // &#039;themesPath&#039; option is ignored.\n    themesPath: &#039;dist/theme/&#039;,\n \n    // Specifies if the transitions menu panel will be shown.\n    // Set to &#039;true&#039; to show the transitions menu panel with\n    // the default transitions list. Alternatively, provide an\n    // array to specify the transitions to make available in\n    // the transitions panel, for example...\n    // [&#039;None&#039;, &#039;Fade&#039;, &#039;Slide&#039;]\n    transitions: false,\n \n    // Adds a menu button to the slides to open the menu panel.\n    // Set to &#039;false&#039; to hide the button.\n    openButton: true,\n \n    // If &#039;true&#039; allows the slide number in the presentation to\n    // open the menu panel. The reveal.js slideNumber option must\n    // be displayed for this to take effect\n    openSlideNumber: false,\n \n    // If true allows the user to open and navigate the menu using\n    // the keyboard. Standard keyboard interaction with reveal\n    // will be disabled while the menu is open.\n    keyboard: true,\n \n    // Normally the menu will close on user actions such as\n    // selecting a menu item, or clicking the presentation area.\n    // If &#039;true&#039;, the sticky option will leave the menu open\n    // until it is explicitly closed, that is, using the close\n    // button or pressing the ESC or m key (when the keyboard\n    // interaction option is enabled).\n    sticky: false,\n \n    // If &#039;true&#039; standard menu items will be automatically opened\n    // when navigating using the keyboard. Note: this only takes\n    // effect when both the &#039;keyboard&#039; and &#039;sticky&#039; options are enabled.\n    autoOpen: true,\n \n    // If &#039;true&#039; the menu will not be created until it is explicitly\n    // requested by calling RevealMenu.init(). Note this will delay\n    // the creation of all menu panels, including custom panels, and\n    // the menu button.\n    delayInit: false,\n \n    // If &#039;true&#039; the menu will be shown when the menu is initialised.\n    openOnInit: false,\n \n    // By default the menu will load it&#039;s own font-awesome library\n    // icons. If your presentation needs to load a different\n    // font-awesome library the &#039;loadIcons&#039; option can be set to false\n    // and the menu will not attempt to load the font-awesome library.\n    loadIcons: true\n  }\n});\nThemes Stylesheet ¬ß\nIf you are using the themes panel you need to ensure the theme stylesheet in the presentation uses the id=&quot;theme&quot; attribute. For example‚Ä¶\n&lt;link rel=&quot;stylesheet&quot; href=&quot;css/theme/black.css&quot; id=&quot;theme&quot; /&gt;\nIf your themes configuration includes code highlight themes you need to ensure the highlights theme stylesheet in the presentation uses the id=&quot;highlight-theme&quot; attribute. For example‚Ä¶\n&lt;link\n  rel=&quot;stylesheet&quot;\n  href=&quot;plugin/highlight/zenburn.css&quot;\n  id=&quot;highlight-theme&quot;\n/&gt;\nSlide Titles ¬ß\nThe slide titles used in the menu can be supplied explicitly or are taken directly from the presentation, using the following rules‚Ä¶\n1. The section‚Äôs data-menu-title attribute. ¬ß\nIf the slide‚Äôs section element contains a data-menu-title attribute this will be used for the slide title in the menu. For example‚Ä¶\n&lt;section data-menu-title=&quot;Custom Menu Title&quot;&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\n2. Any element with the class menu-title. ¬ß\nIf the slide‚Äôs section contains an element with the class menu-title then the element‚Äôs text will be used for the title. The first such element found will be used if there are more than one. Note the element need not be displayed to be used. For example‚Ä¶\n&lt;section&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;span class=&quot;menu-title&quot; style=&quot;display: none&quot;&gt;Custom Menu Title&lt;/span&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\n3. The first heading found or a custom element selector ¬ß\nThe titleSelector option can be used to customise the elements that will be used to generate the slide titles in the menu. The default option selects the first heading element found in the slide. For example‚Ä¶\n&lt;section&gt;\n  &lt;h3&gt;This will be the slide title in the menu&lt;/h3&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nAny valid CSS selector should work but note the selector will only be applied to elements contained within the slide section. You could use the &#039;h1&#039; selector to only use level 1 headings or &#039;p&#039; to use the first paragraph element. For example, titleSelector: &#039;p.lead&#039; would be used like this‚Ä¶\n&lt;section&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p class=&quot;lead&quot;&gt;This will be the slide title in the menu&lt;/p&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nUsing titleSelector: &#039;&#039; will ignore all elements and no title will be provided, unless the slide section contains a data-menu-title attribute or an element with the menu-title class.\n4. No title is provided ¬ß\nIf no title can be found using the above methods, a default title incorporating the slide number will be used. For example, the following would result in a slide title in the format of ‚ÄòSlide 12‚Äô‚Ä¶\n&lt;section&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nIf the hideMissingTitles option is set to true, however, the slide will not be listed in the menu.\nCustom Menu Panels ¬ß\nAdditional custom panels can be added the menu using the custom option.\nReveal.initialize({\n  // ...\n \n  menu: {\n    // ...\n \n    custom: [\n      {\n        title: &#039;Links&#039;,\n        icon: &#039;&lt;i class=&quot;fa fa-external-link&quot;&gt;&#039;,\n        src: &#039;links.html&#039;\n      },\n      {\n        title: &#039;About&#039;,\n        icon: &#039;&lt;i class=&quot;fa fa-info&quot;&gt;&#039;,\n        content: &#039;&lt;p&gt;This slidedeck is created with reveal.js&lt;/p&gt;&#039;\n      }\n    ]\n  }\n});\ntitle and icon are used for the toolbar buttons at the top of the menu. There are two approaches you can use to provide content for the panels‚Ä¶\n\nYou can provide a URL in src to load html from another file.\nAlternatively, you can provide html in content and this will be added to the custom panel.\n\nCustom slide menu items ¬ß\nYou can provide menu items in your custom panels using the following format. This allows you to define your own navigation links for your presentation.\n&lt;h1&gt;Links&lt;/h1&gt;\n&lt;ul class=&quot;slide-menu-items&quot;&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;&lt;a href=&quot;#/transitions&quot;&gt;Transitions&lt;/a&gt;&lt;/li&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;&lt;a href=&quot;#/13&quot;&gt;Code highlighting&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\nYou are not limited to linking to presentation slides. You can provide any link you wish.\n&lt;h1&gt;External Links&lt;/h1&gt;\n&lt;ul class=&quot;slide-menu-items&quot;&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;\n    &lt;a href=&quot;https://github.com/denehyg/reveal.js-menu&quot;&gt;Reveal.js-menu&lt;/a&gt;\n  &lt;/li&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;\n    &lt;a href=&quot;https://github.com/hakimel/reveal.js&quot;&gt;Reveal.js&lt;/a&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\nUsing menu items enables keyboard navigation of your links as with the other panels. However, you don‚Äôt have to use menu items for your links. You can simply provide standard links and unordered lists in your html. Notice you can provide your custom menu items mixed with other html if you wish.\nReady Event ¬ß\nA ‚Äòmenu-ready‚Äô event is fired when reveal.js-menu has loaded all non-async dependencies and is ready to start navigating.\nReveal.addEventListener(&#039;menu-ready&#039;, function (event) {\n  // your code\n});\nAPI ¬ß\nThe RevealMenu object exposes a JavaScript API for controlling the menu:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctionDescriptiontoggle(event)Toggles the open state of the menu, ie open if it is closed, and close if it is openopenMenu(event)Opens the menucloseMenu(event, force)Closes the menu. To force the menu to close (ie when sticky option is true) call closeMenu(null, true)openPanel(event, ref)Opens the menu to a specific panel, passing the name of the panel or the panel element itselfisOpen()Returns true if the menu is openinitialiseMenu()Initialises the menu if it has not already been initialised. Used in conjunction with the delayInit optionisMenuInitialised()Returns true if the menu has been initialised\nCompatibility ¬ß\nreveal.js-menu v2.0 is built for reveal.js v4. It will not work with reveal.js v3. If you require a menu for reveal.js v3 you will need to install reveal.js-menu v1.2.0.\nv2.0 also introduces API changes that are not backwards compatible. init() has been renamed to initMenu() to deconflict with the reveal.js v4 plugin API. isInit() has also been changed to isMenuInitialised().\nLicense ¬ß\nMIT licensed\nCopyright (C) 2020 Greg Denehy"},"public_notes/export/creanalytics/plugin/chalkboard/README":{"title":"README","links":[],"tags":[],"content":"Chalkboard ¬ß\nWith this plugin you can add a chalkboard to reveal.js. The plugin provides two possibilities to include handwritten notes to your presentation:\n\nyou can make notes directly on the slides, e.g. to comment on certain aspects,\nyou can open a chalkboard or whiteboard on which you can make notes.\n\nThe main use case in mind when implementing the plugin is classroom usage in which you may want to explain some course content and quickly need to make some notes.\nThe plugin records all drawings made so that they can be play backed using the autoSlide feature or the audio-slideshow plugin.\nCheck out the live demo\nThe chalkboard effect is based on Chalkboard by Mohamed Moustafa.\nInstallation ¬ß\nCopy the file plugin.js and the  img directory into the plugin folder of your reveal.js presentation, i.e. plugin/chalkboard and load the plugin as shown below.\n&lt;script src=&quot;plugin/chalkboard/plugin.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;plugin/chalkboard/customcontrols.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealChalkboard, RevealCustomControls ],\n        // ...\n    });\n&lt;/script&gt;\nThe following stylesheet\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/chalkboard/style.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/customcontrols/style.css&quot;&gt;\nhas to be included to the head section of you HTML-file.\nIn order to include buttons for opening and closing the notes canvas or the chalkboard you should make sure that font-awesome is available. The easiest way is to include\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css&quot;&gt;\n\nto the head section of you HTML-file.\nUsage ¬ß\nMouse or touch ¬ß\n\nClick on the pen symbols at the bottom left to toggle the notes canvas or chalkboard\nClick on the color picker at the left to change the color (the color picker is only visible if the notes canvas or chalkboard is active)\nClick on the up/down arrows on the left to the switch among multiple chalkboardd (the up/down arrows are only available for the chlakboard)\nClick the left mouse button and drag to write on notes canvas or chalkboard\nClick the right mouse button and drag to wipe away previous drawings\nTouch and move to write on notes canvas or chalkboard\nTouch and hold for half a second, then move to wipe away previous drawings\n\nKeyboard ¬ß\n\nPress the ‚ÄòBACKSPACE‚Äô key to delete all chalkboard drawings\nPress the ‚ÄòDEL‚Äô key to clear the notes canvas or chalkboard\nPress the ‚Äòc‚Äô key to toggle the notes canvas\nPress the ‚Äòb‚Äô key to toggle the chalkboard\nPress the ‚Äòd‚Äô key to download drawings\nPress the ‚Äòx‚Äô key to cycle colors forward\nPress the ‚Äòy‚Äô key to cycle colors backward\n\nPlayback ¬ß\nIf the autoSlide feature is set or if the audio-slideshow plugin is used, pre-recorded chalkboard drawings can be played. The slideshow plays back the user interaction with the chalkboard in the same way as it was conducted when recording the data.\nMultiplexing ¬ß\nThe plugin supports multiplexing via the multiplex plugin or the seminar plugin.\nPDF-Export ¬ß\nIf the slideshow is opened in print mode, the chalkboard drawings in the session storage (see storage option - print version must be opened in the same tab or window as the original slideshow) or provided in a file (see src option) are included in the PDF-file. Each drawing on the chalkboard is added after the slide that was shown when opening the chalkboard. Drawings on the notes canvas are not included in the PDF-file.\nConfiguration ¬ß\nThe plugin has several configuration options:\n\nboardmarkerWidth: an integer, the drawing width of the boardmarker; larger values draw thicker lines.\nchalkWidth: an integer, the drawing width of the chalk; larger values draw thicker lines.\nchalkEffect: a float in the range [0.0, 1.0], the intesity of the chalk effect on the chalk board. Full effect (default) 1.0, no effect 0.0.\nstorage: Optional variable name for session storage of drawings.\nsrc: Optional filename for pre-recorded drawings.\nreadOnly: Configuation option allowing to prevent changes to existing drawings. If set to true no changes can be made, if set to false false changes can be made, if unset or set to undefined no changes to the drawings can be made after returning to a slide or fragment for which drawings had been recorded before. In any case the recorded drawings for a slide or fragment can be cleared by pressing the ‚ÄòDEL‚Äô key (i.e. by using the RevealChalkboard.clear() function).\ntransition: Gives the duration (in milliseconds) of the transition for a slide change, so that the notes canvas is drawn after the transition is completed.\ntheme: Can be set to either &quot;chalkboard&quot; or &quot;whiteboard&quot;.\n\nThe following configuration options allow to change the appearance of the notes canvas and the chalkboard. All of these options require two values, the first gives the value for the notes canvas, the second for the chalkboard.\n\nbackground: The first value expects a (semi-)transparent color which is used to provide visual feedback that the notes canvas is enabled, the second value expects a filename to a background image for the chalkboard.\ngrid: By default whiteboard and chalkboard themes include a grid pattern on the background. This pattern can be modified by setting the color, the distance between lines, and the line width, e.g. { color: &#039;rgb(127,127,255,0.1)&#039;, distance: 40, width: 2}. Alternatively, the grid can be removed by setting the value to false.\neraser: An image path and radius for the eraser.\nboardmarkers: A list of boardmarkers with given color and cursor.\nchalks: A list of chalks with given color and cursor.\nrememberColor: Whether to remember the last selected color for the slide canvas or the board.\n\nAll of the configurations are optional and the default values shown below are used if the options are not provided.\nReveal.initialize({\n\t// ...\n    chalkboard: {\n        boardmarkerWidth: 3,\n        chalkWidth: 7,\n        chalkEffect: 1.0,\n        storage: null,\n        src: null,\n        readOnly: undefined,\n        transition: 800,\n        theme: &quot;chalkboard&quot;,\n        background: [ &#039;rgba(127,127,127,.1)&#039; , path + &#039;img/blackboard.png&#039; ],\n        grid: { color: &#039;rgb(50,50,10,0.5)&#039;, distance: 80, width: 2},\n        eraser: { src: path + &#039;img/sponge.png&#039;, radius: 20},\n        boardmarkers : [\n                { color: &#039;rgba(100,100,100,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-black.png), auto&#039;},\n                { color: &#039;rgba(30,144,255, 1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-blue.png), auto&#039;},\n                { color: &#039;rgba(220,20,60,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-red.png), auto&#039;},\n                { color: &#039;rgba(50,205,50,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-green.png), auto&#039;},\n                { color: &#039;rgba(255,140,0,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-orange.png), auto&#039;},\n                { color: &#039;rgba(150,0,20150,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-purple.png), auto&#039;},\n                { color: &#039;rgba(255,220,0,1)&#039;, cursor: &#039;url(&#039; + path + &#039;img/boardmarker-yellow.png), auto&#039;}\n        ],\n        chalks: [\n                { color: &#039;rgba(255,255,255,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-white.png), auto&#039;},\n                { color: &#039;rgba(96, 154, 244, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-blue.png), auto&#039;},\n                { color: &#039;rgba(237, 20, 28, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-red.png), auto&#039;},\n                { color: &#039;rgba(20, 237, 28, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-green.png), auto&#039;},\n                { color: &#039;rgba(220, 133, 41, 0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-orange.png), auto&#039;},\n                { color: &#039;rgba(220,0,220,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-purple.png), auto&#039;},\n                { color: &#039;rgba(255,220,0,0.5)&#039;, cursor: &#039;url(&#039; + path + &#039;img/chalk-yellow.png), auto&#039;}\n        ]\n    },\n    customcontrols: {\n  \t\tcontrols: [\n  \t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen-square&quot;&gt;&lt;/i&gt;&#039;,\n  \t\t\t  title: &#039;Toggle chalkboard (B)&#039;,\n  \t\t\t  action: &#039;RevealChalkboard.toggleChalkboard();&#039;\n  \t\t\t},\n  \t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen&quot;&gt;&lt;/i&gt;&#039;,\n  \t\t\t  title: &#039;Toggle notes canvas (C)&#039;,\n  \t\t\t  action: &#039;RevealChalkboard.toggleNotesCanvas();&#039;\n  \t\t\t}\n  \t\t]\n    },\n    // ...\n \n});\nLicense ¬ß\nMIT licensed\nCopyright (C) 2021 Asvin Goel"},"public_notes/export/creanalytics/plugin/chart/README":{"title":"README","links":[],"tags":[],"content":"Chart ¬ß\nA plugin for Reveal.js allowing to easily add charts using Chart.js.\nCheck out the live demo\nInstallation ¬ß\nCopy the file plugin.js into the plugin folder of your reveal.js presentation, i.e. plugin/chart.\nAdd the plugin and Chart.js to the dependencies in your presentation, as below.\n&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.2.0/chart.min.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;plugin/chart/plugin.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealChart ],\n        // ...\n    });\n&lt;/script&gt;\nConfiguration ¬ß\nThe plugin has several parameters that you can set for your presentation by providing an chart option in the reveal.js initialization options. Note that all configuration parameters are optional and the defaults of Chart.js will be used for parameters that are not specified.\nReveal.initialize({\n\t// ...\n\tchart: {\n\t\tdefaults: {\n\t\t\tcolor: &#039;lightgray&#039;, // color of labels\n\t\t\tscale: {\n\t\t\t\tbeginAtZero: true,\n\t\t\t\tticks: { stepSize: 1 },\n\t\t\t\tgrid: { color: &quot;lightgray&quot; } , // color of grid lines\n\t\t\t},\n\t\t},\n\t\tline: { borderColor: [ &quot;rgba(20,220,220,.8)&quot; , &quot;rgba(220,120,120,.8)&quot;, &quot;rgba(20,120,220,.8)&quot; ], &quot;borderDash&quot;: [ [5,10], [0,0] ] },\n\t\tbar: { backgroundColor: [ &quot;rgba(20,220,220,.8)&quot; , &quot;rgba(220,120,120,.8)&quot;, &quot;rgba(20,120,220,.8)&quot; ]},\n\t\tpie: { backgroundColor: [ [&quot;rgba(0,0,0,.8)&quot; , &quot;rgba(220,20,20,.8)&quot;, &quot;rgba(20,220,20,.8)&quot;, &quot;rgba(220,220,20,.8)&quot;, &quot;rgba(20,20,220,.8)&quot;] ]},\n\t},\n        // ...\n});\nThe defaults parameter  will overwrite Chart.defaults. Furthermore, for any chart type, e.g. line, bar, etc., the parameters for the individual datasets can be specified. Where Chart.js allows to specify a single parameter for a particular dataset, the plugin allows to specify an array of values for this parameter, which will automatically be assigned to the different datasets. Note that if there are more datasets than elements in the array, the plugin will start again with the first value in the array.\nUsage ¬ß\nA chart can be included in a slide by adding a canvas element with the data-chart attribute set to the desired chart type.\nThe chart can be configured within the canvas body by a JSON string embedded into an HTML comment.\n&lt;canvas data-chart=&quot;line&quot; &gt;\n&lt;!--\n{\n &quot;data&quot;: {\n  &quot;labels&quot;: [&quot;January&quot;,&quot; February&quot;,&quot; March&quot;,&quot; April&quot;,&quot; May&quot;,&quot; June&quot;,&quot; July&quot;],\n  &quot;datasets&quot;:[\n   {\n    &quot;data&quot;:[65,59,80,81,56,55,40],\n    &quot;label&quot;:&quot;My first dataset&quot;,&quot;backgroundColor&quot;:&quot;rgba(20,220,220,.8)&quot;\n   },\n   {\n    &quot;data&quot;:[28,48,40,19,86,27,90],\n    &quot;label&quot;:&quot;My second dataset&quot;,&quot;backgroundColor&quot;:&quot;rgba(220,120,120,.8)&quot;\n   }\n  ]\n }\n}\n--&gt;\n&lt;/canvas&gt;\nIt is possible to provide the chart data by comma separated values and use the JSON string within the HTML comment to configure the chart layout.\n&lt;canvas class=&quot;stretch&quot; data-chart=&quot;line&quot;&gt;\nMy first dataset, 65, 59, 80, 81, 56, 55, 40\n&lt;!-- This is a comment that will be ignored --&gt;\nMy second dataset, 28, 48, 40, 19, 86, 27, 90\n&lt;!--\n{\n &quot;data&quot; : {\n  &quot;labels&quot; : [&quot;Enero&quot;, &quot;Febrero&quot;, &quot;Marzo&quot;, &quot;Avril&quot;, &quot;Mayo&quot;, &quot;Junio&quot;, &quot;Julio&quot;],\n  &quot;datasets&quot; : [{ &quot;borderColor&quot;: &quot;#0f0&quot;, &quot;borderDash&quot;: [&quot;5&quot;,&quot;10&quot;] }, { &quot;borderColor&quot;: &quot;#0ff&quot; } ]\n }\n}\n--&gt;\n&lt;/canvas&gt;\nThe layout configuration provided in chart parameter (see Configuration) will be used by default and only those parameters that are specified in a JSON string are used to overwrite the default values. If no JSON string is provided to configure the chart layout the default configuration is used. Note, that if no labels for the data points are provided by a JSON string, the plugin expects that the first row provides table headers.\n&lt;canvas data-chart=&quot;line&quot;&gt;\nMonth, January, February, March, April, May, June, July\nMy first dataset, 65, 59, 80, 81, 56, 55, 40\nMy second dataset, 28, 48, 40, 19, 86, 27, 90\n&lt;/canvas&gt;\nThe chart data can also be provided in an external CSV file. To include external data, the filename must be specified using the data-chart-src attribute of the canvas element. The CSV file is expected to only contain data values, whereas options for drawing the chart can be given as described above.\n&lt;canvas data-chart=&quot;bar&quot; data-chart-src=&quot;chart/data.csv&quot;&gt;\n&lt;!--\n{\n&quot;data&quot; : {\n&quot;datasets&quot; : [{ &quot;backgroundColor&quot;: &quot;#0f0&quot; }, { &quot;backgroundColor&quot;: &quot;#0ff&quot; } ]\n},\n&quot;options&quot;: { &quot;scales&quot;: { &quot;x&quot;: { &quot;stacked&quot;: true }, &quot;y&quot;: { &quot;stacked&quot;: true } } }\n}\n--&gt;\n&lt;/canvas&gt;\nLicense ¬ß\nMIT licensed\nCopyright (C) 2021 Asvin Goel"},"public_notes/export/creanalytics/plugin/customcontrols/README":{"title":"README","links":[],"tags":[],"content":"Custom controls ¬ß\nThis plugin allows to add responsive custom controls to reveal.js which allow arbitrary positioning, layout, and behaviour of the controls.\nCheck out the live demo\nInstallation ¬ß\nCopy the files plugin.js and style.css into the plugin folder of your reveal.js presentation, i.e. plugin/customcontrols and load the plugin as shown below.\n&lt;link rel=&quot;stylesheet&quot; href=&quot;plugin/customcontrols/style.css&quot;&gt;\n&lt;script src=&quot;plugin/customcontrols/plugin.js&quot;&gt;&lt;/script&gt;\n \n&lt;script&gt;\n    Reveal.initialize({\n        // ...\n        plugins: [ RevealCustomControls ],\n        // ...\n    });\n&lt;/script&gt;\nNote, without configuration you need to add\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css&quot;&gt;\nbetween &lt;head&gt; and &lt;/head&gt; of your HTML file because the defaults use Font Awesome.\nConfiguration ¬ß\nThe plugin can be configured by adding custom controls and changing the layout of the slide number, e.g., by:\nReveal.initialize({\n\t// ...\n  customcontrols: {\n\t\tcontrols: [\n      {\n\t\t\t  id: &#039;toggle-overview&#039;,\n\t\t\t  title: &#039;Toggle overview (O)&#039;,\n\t\t\t  icon: &#039;&lt;i class=&quot;fa fa-th&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  action: &#039;Reveal.toggleOverview();&#039;\n\t\t\t},\n\t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen-square&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  title: &#039;Toggle chalkboard (B)&#039;,\n\t\t\t  action: &#039;RevealChalkboard.toggleChalkboard();&#039;\n\t\t\t},\n\t\t\t{ icon: &#039;&lt;i class=&quot;fa fa-pen&quot;&gt;&lt;/i&gt;&#039;,\n\t\t\t  title: &#039;Toggle notes canvas (C)&#039;,\n\t\t\t  action: &#039;RevealChalkboard.toggleNotesCanvas();&#039;\n\t\t\t}\n\t\t]\n\t},\n\t// ...\n \n});\nThe id and title are optional. The configuration should be self explaining and any number of controls can be added. The style file can be altered to control the layout and responsiveness of the custom controls.\nLicense ¬ß\nMIT licensed\nCopyright (C) 2020 Asvin Goel"},"public_notes/export/creanalytics/plugin/menu/CONTRIBUTING":{"title":"CONTRIBUTING","links":[],"tags":[],"content":"Contributing ¬ß\nBug Reports ¬ß\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\nPull Requests ¬ß\n\nShould follow the coding style of the file you work in\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n"},"public_notes/export/creanalytics/plugin/menu/README":{"title":"README","links":[],"tags":[],"content":"reveal.js-menu ¬ß\nA slideout menu plugin for Reveal.js to quickly jump to any slide by title. Also optionally change the theme and set the default transition. Check out the live demo\nInstallation ¬ß\nBower ¬ß\nDownload and install the package in your project:\nbower install reveal.js-menu\nAdd the plugin to your presentation, as below.\n&lt;script src=&quot;bower_components/reveal.js-menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nnpm ¬ß\nDownload and install the package in your project:\nnpm install --save reveal.js-menu\nAdd the plugin to your presentation, as below.\n&lt;script src=&quot;node_modules/reveal.js-menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nManual ¬ß\nCopy this repository into the plugins folder of your reveal.js presentation, ie plugins/menu.\nAdd the plugin to the dependencies in your presentation, as below.\n&lt;script src=&quot;plugin/menu/menu.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMenu ]\n  });\n&lt;/script&gt;\nConfiguration ¬ß\nYou can configure the menu for your presentation by providing a menu option in the reveal.js initialization options. Note that all config values are optional and will default as specified below.\nReveal.initialize({\n  // ...\n \n  menu: {\n    // Specifies which side of the presentation the menu will\n    // be shown. Use &#039;left&#039; or &#039;right&#039;.\n    side: &#039;left&#039;,\n \n    // Specifies the width of the menu.\n    // Can be one of the following:\n    // &#039;normal&#039;, &#039;wide&#039;, &#039;third&#039;, &#039;half&#039;, &#039;full&#039;, or\n    // any valid css length value\n    width: &#039;normal&#039;,\n \n    // Add slide numbers to the titles in the slide list.\n    // Use &#039;true&#039; or format string (same as reveal.js slide numbers)\n    numbers: false,\n \n    // Specifies which slide elements will be used for generating\n    // the slide titles in the menu. The default selects the first\n    // heading element found in the slide, but you can specify any\n    // valid css selector and the text from the first matching\n    // element will be used.\n    // Note: that a section data-menu-title attribute or an element\n    // with a menu-title class will take precedence over this option\n    titleSelector: &#039;h1, h2, h3, h4, h5, h6&#039;,\n \n    // If slides do not have a matching title, attempt to use the\n    // start of the text content as the title instead\n    useTextContentForMissingTitles: false,\n \n    // Hide slides from the menu that do not have a title.\n    // Set to &#039;true&#039; to only list slides with titles.\n    hideMissingTitles: false,\n \n    // Adds markers to the slide titles to indicate the\n    // progress through the presentation. Set to &#039;false&#039;\n    // to hide the markers.\n    markers: true,\n \n    // Specify custom panels to be included in the menu, by\n    // providing an array of objects with &#039;title&#039;, &#039;icon&#039;\n    // properties, and either a &#039;src&#039; or &#039;content&#039; property.\n    custom: false,\n \n    // Specifies the themes that will be available in the themes\n    // menu panel. Set to &#039;true&#039; to show the themes menu panel\n    // with the default themes list. Alternatively, provide an\n    // array to specify the themes to make available in the\n    // themes menu panel, for example...\n    //\n    // [\n    //     { name: &#039;Black&#039;, theme: &#039;dist/theme/black.css&#039; },\n    //     { name: &#039;White&#039;, theme: &#039;dist/theme/white.css&#039; },\n    //     { name: &#039;League&#039;, theme: &#039;dist/theme/league.css&#039; },\n    //     {\n    //       name: &#039;Dark&#039;,\n    //       theme: &#039;lib/reveal.js/dist/theme/black.css&#039;,\n    //       highlightTheme: &#039;lib/reveal.js/plugin/highlight/monokai.css&#039;\n    //     },\n    //     {\n    //       name: &#039;Code: Zenburn&#039;,\n    //       highlightTheme: &#039;lib/reveal.js/plugin/highlight/zenburn.css&#039;\n    //     }\n    // ]\n    //\n    // Note: specifying highlightTheme without a theme will\n    // change the code highlight theme while leaving the\n    // presentation theme unchanged.\n    themes: false,\n \n    // Specifies the path to the default theme files. If your\n    // presentation uses a different path to the standard reveal\n    // layout then you need to provide this option, but only\n    // when &#039;themes&#039; is set to &#039;true&#039;. If you provide your own\n    // list of themes or &#039;themes&#039; is set to &#039;false&#039; the\n    // &#039;themesPath&#039; option is ignored.\n    themesPath: &#039;dist/theme/&#039;,\n \n    // Specifies if the transitions menu panel will be shown.\n    // Set to &#039;true&#039; to show the transitions menu panel with\n    // the default transitions list. Alternatively, provide an\n    // array to specify the transitions to make available in\n    // the transitions panel, for example...\n    // [&#039;None&#039;, &#039;Fade&#039;, &#039;Slide&#039;]\n    transitions: false,\n \n    // Adds a menu button to the slides to open the menu panel.\n    // Set to &#039;false&#039; to hide the button.\n    openButton: true,\n \n    // If &#039;true&#039; allows the slide number in the presentation to\n    // open the menu panel. The reveal.js slideNumber option must\n    // be displayed for this to take effect\n    openSlideNumber: false,\n \n    // If true allows the user to open and navigate the menu using\n    // the keyboard. Standard keyboard interaction with reveal\n    // will be disabled while the menu is open.\n    keyboard: true,\n \n    // Normally the menu will close on user actions such as\n    // selecting a menu item, or clicking the presentation area.\n    // If &#039;true&#039;, the sticky option will leave the menu open\n    // until it is explicitly closed, that is, using the close\n    // button or pressing the ESC or m key (when the keyboard\n    // interaction option is enabled).\n    sticky: false,\n \n    // If &#039;true&#039; standard menu items will be automatically opened\n    // when navigating using the keyboard. Note: this only takes\n    // effect when both the &#039;keyboard&#039; and &#039;sticky&#039; options are enabled.\n    autoOpen: true,\n \n    // If &#039;true&#039; the menu will not be created until it is explicitly\n    // requested by calling RevealMenu.init(). Note this will delay\n    // the creation of all menu panels, including custom panels, and\n    // the menu button.\n    delayInit: false,\n \n    // If &#039;true&#039; the menu will be shown when the menu is initialised.\n    openOnInit: false,\n \n    // By default the menu will load it&#039;s own font-awesome library\n    // icons. If your presentation needs to load a different\n    // font-awesome library the &#039;loadIcons&#039; option can be set to false\n    // and the menu will not attempt to load the font-awesome library.\n    loadIcons: true\n  }\n});\nThemes Stylesheet ¬ß\nIf you are using the themes panel you need to ensure the theme stylesheet in the presentation uses the id=&quot;theme&quot; attribute. For example‚Ä¶\n&lt;link rel=&quot;stylesheet&quot; href=&quot;css/theme/black.css&quot; id=&quot;theme&quot; /&gt;\nIf your themes configuration includes code highlight themes you need to ensure the highlights theme stylesheet in the presentation uses the id=&quot;highlight-theme&quot; attribute. For example‚Ä¶\n&lt;link\n  rel=&quot;stylesheet&quot;\n  href=&quot;plugin/highlight/zenburn.css&quot;\n  id=&quot;highlight-theme&quot;\n/&gt;\nSlide Titles ¬ß\nThe slide titles used in the menu can be supplied explicitly or are taken directly from the presentation, using the following rules‚Ä¶\n1. The section‚Äôs data-menu-title attribute. ¬ß\nIf the slide‚Äôs section element contains a data-menu-title attribute this will be used for the slide title in the menu. For example‚Ä¶\n&lt;section data-menu-title=&quot;Custom Menu Title&quot;&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\n2. Any element with the class menu-title. ¬ß\nIf the slide‚Äôs section contains an element with the class menu-title then the element‚Äôs text will be used for the title. The first such element found will be used if there are more than one. Note the element need not be displayed to be used. For example‚Ä¶\n&lt;section&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;span class=&quot;menu-title&quot; style=&quot;display: none&quot;&gt;Custom Menu Title&lt;/span&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\n3. The first heading found or a custom element selector ¬ß\nThe titleSelector option can be used to customise the elements that will be used to generate the slide titles in the menu. The default option selects the first heading element found in the slide. For example‚Ä¶\n&lt;section&gt;\n  &lt;h3&gt;This will be the slide title in the menu&lt;/h3&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nAny valid CSS selector should work but note the selector will only be applied to elements contained within the slide section. You could use the &#039;h1&#039; selector to only use level 1 headings or &#039;p&#039; to use the first paragraph element. For example, titleSelector: &#039;p.lead&#039; would be used like this‚Ä¶\n&lt;section&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p class=&quot;lead&quot;&gt;This will be the slide title in the menu&lt;/p&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nUsing titleSelector: &#039;&#039; will ignore all elements and no title will be provided, unless the slide section contains a data-menu-title attribute or an element with the menu-title class.\n4. No title is provided ¬ß\nIf no title can be found using the above methods, a default title incorporating the slide number will be used. For example, the following would result in a slide title in the format of ‚ÄòSlide 12‚Äô‚Ä¶\n&lt;section&gt;\n  &lt;p&gt;...&lt;/p&gt;\n&lt;/section&gt;\nIf the hideMissingTitles option is set to true, however, the slide will not be listed in the menu.\nCustom Menu Panels ¬ß\nAdditional custom panels can be added the menu using the custom option.\nReveal.initialize({\n  // ...\n \n  menu: {\n    // ...\n \n    custom: [\n      {\n        title: &#039;Links&#039;,\n        icon: &#039;&lt;i class=&quot;fa fa-external-link&quot;&gt;&#039;,\n        src: &#039;links.html&#039;\n      },\n      {\n        title: &#039;About&#039;,\n        icon: &#039;&lt;i class=&quot;fa fa-info&quot;&gt;&#039;,\n        content: &#039;&lt;p&gt;This slidedeck is created with reveal.js&lt;/p&gt;&#039;\n      }\n    ]\n  }\n});\ntitle and icon are used for the toolbar buttons at the top of the menu. There are two approaches you can use to provide content for the panels‚Ä¶\n\nYou can provide a URL in src to load html from another file.\nAlternatively, you can provide html in content and this will be added to the custom panel.\n\nCustom slide menu items ¬ß\nYou can provide menu items in your custom panels using the following format. This allows you to define your own navigation links for your presentation.\n&lt;h1&gt;Links&lt;/h1&gt;\n&lt;ul class=&quot;slide-menu-items&quot;&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;&lt;a href=&quot;#/transitions&quot;&gt;Transitions&lt;/a&gt;&lt;/li&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;&lt;a href=&quot;#/13&quot;&gt;Code highlighting&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\nYou are not limited to linking to presentation slides. You can provide any link you wish.\n&lt;h1&gt;External Links&lt;/h1&gt;\n&lt;ul class=&quot;slide-menu-items&quot;&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;\n    &lt;a href=&quot;https://github.com/denehyg/reveal.js-menu&quot;&gt;Reveal.js-menu&lt;/a&gt;\n  &lt;/li&gt;\n  &lt;li class=&quot;slide-menu-item&quot;&gt;\n    &lt;a href=&quot;https://github.com/hakimel/reveal.js&quot;&gt;Reveal.js&lt;/a&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\nUsing menu items enables keyboard navigation of your links as with the other panels. However, you don‚Äôt have to use menu items for your links. You can simply provide standard links and unordered lists in your html. Notice you can provide your custom menu items mixed with other html if you wish.\nReady Event ¬ß\nA ‚Äòmenu-ready‚Äô event is fired when reveal.js-menu has loaded all non-async dependencies and is ready to start navigating.\nReveal.addEventListener(&#039;menu-ready&#039;, function (event) {\n  // your code\n});\nAPI ¬ß\nThe RevealMenu object exposes a JavaScript API for controlling the menu:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctionDescriptiontoggle(event)Toggles the open state of the menu, ie open if it is closed, and close if it is openopenMenu(event)Opens the menucloseMenu(event, force)Closes the menu. To force the menu to close (ie when sticky option is true) call closeMenu(null, true)openPanel(event, ref)Opens the menu to a specific panel, passing the name of the panel or the panel element itselfisOpen()Returns true if the menu is openinitialiseMenu()Initialises the menu if it has not already been initialised. Used in conjunction with the delayInit optionisMenuInitialised()Returns true if the menu has been initialised\nCompatibility ¬ß\nreveal.js-menu v2.0 is built for reveal.js v4. It will not work with reveal.js v3. If you require a menu for reveal.js v3 you will need to install reveal.js-menu v1.2.0.\nv2.0 also introduces API changes that are not backwards compatible. init() has been renamed to initMenu() to deconflict with the reveal.js v4 plugin API. isInit() has also been changed to isMenuInitialised().\nLicense ¬ß\nMIT licensed\nCopyright (C) 2020 Greg Denehy"},"public_notes/small-large-LLMs":{"title":"small large LLMs","links":[],"tags":["llms","tools"],"content":"\nMy colleague Mercedes Bunz made me aware of this allegedly leaked document from a Google engineer, in which they make the case for open-sourcing their models. TLDR; The argument is that owning and cultivating the ecosystem for innovation is more valuable than keeping the models fenced off.\nCaveats notwithstanding, say for instance that the diminishing value of training does not account for people‚Äôs salaries in publicly-funded institutions, it is still an interesting read, especially the timeline narrating all the developments. Good for teaching, but also to make sense of the various recent moves in the field.\nSome of the models and mentioned:\nLLaMA ‚Äï Meta\nAlpaca‚Äï Stanford\nAlpaca LoRA‚Äï Stanford + Eric Wang. See paper here.\nA Chatbot interface for Alpaca\nDolly 15k instructions dataset\n\ndatabricks-dolly-15k is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.\n\nGPT4all‚Äï open stack pipeline based on JPT-J and LLaMA\nVicuna\n\nan open-source Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT\n\nThis las one was developed by a student-led university consortium called LMSYS Org!"}}